\relax 
\citation{cheng2020remote,ma2019deep}
\citation{yang2010bag}
\citation{cheng2017nwpu}
\citation{simonyan2015very}
\citation{he2016deep}
\citation{huang2017densely}
\citation{deng2009imagenet}
\citation{nogueira2017towards,li2018deep}
\citation{pan2010survey,neumann2019domain}
\citation{vaswani2017attention}
\citation{dosovitskiy2021image}
\citation{liu2021swin}
\citation{hong2022spectralformer,wang2023advancing}
\citation{liu2022convnet}
\citation{mcnemar1947note}
\citation{he2016deep}
\citation{huang2017densely}
\citation{tan2019efficientnet}
\citation{nogueira2017towards}
\citation{ma2019deep,zhu2017deep}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{}\protected@file@percent }
\citation{dosovitskiy2021image}
\citation{wang2023advancing}
\citation{liu2021swin}
\citation{hong2022spectralformer}
\citation{liu2022convnet}
\citation{yang2010bag}
\citation{helber2019eurosat}
\citation{cheng2017nwpu}
\citation{xia2017aid}
\citation{helber2019eurosat}
\citation{yang2010bag}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}CNN-Based Scene Classification}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}Transformer-Based Approaches}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-C}}Modernized CNNs}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-D}}Benchmark Datasets}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Methodology}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Datasets}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-A}1}EuroSAT}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-A}2}UC Merced Land Use}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-A}3}Data Partitioning}{2}{}\protected@file@percent }
\citation{he2016deep}
\citation{he2016deep}
\citation{huang2017densely}
\citation{tan2019efficientnet}
\citation{tan2019efficientnet}
\citation{dosovitskiy2021image}
\citation{liu2021swin}
\citation{liu2022convnet}
\citation{wightman2019timm}
\citation{loshchilov2019adamw}
\citation{paszke2019pytorch}
\citation{cohen1960coefficient}
\citation{congalton1991review,foody2002status}
\citation{mcnemar1947note,dietterich1998approximate}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Overview of the research methodology. Both datasets pass through the same preprocessing and train all eight architectures under identical conditions. Evaluation covers accuracy, McNemar significance testing, confusion analysis, and training cost.}}{3}{}\protected@file@percent }
\newlabel{fig:methodology}{{1}{3}{}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Model Architectures}{3}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Model architectures evaluated in this study. Parameter counts refer to the ImageNet-pretrained backbone before replacing the classifier head.}}{3}{}\protected@file@percent }
\newlabel{tab:models}{{I}{3}{}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Training Protocol}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-D}}Evaluation Metrics}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-D}1}Classification Metrics}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-D}2}Statistical Significance}{3}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Dataset samples. Left: one randomly selected Sentinel-2 patch per class from EuroSAT (64$\times $64 pixels, 10\,m resolution, 10 classes). Right: one randomly selected aerial image per class from UC Merced (256$\times $256 pixels, 0.3\,m resolution, 21 classes).}}{4}{}\protected@file@percent }
\newlabel{fig:samples}{{2}{4}{}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Data augmentation pipeline. The leftmost column shows original EuroSAT images resized to 224$\times $224 pixels. The two columns to the right show augmented versions produced by random flips, rotation, and color jitter.}}{5}{}\protected@file@percent }
\newlabel{fig:augmentation}{{3}{5}{}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-D}3}Computational Efficiency}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Overall Performance}{5}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Classification performance on EuroSAT (5{,}400 test images). Best results in bold.}}{5}{}\protected@file@percent }
\newlabel{tab:eurosat}{{II}{5}{}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Classification performance on UC Merced (420 test images). Best results in bold.}}{5}{}\protected@file@percent }
\newlabel{tab:ucmerced}{{III}{5}{}{table.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Accuracy comparison across both datasets. All models exceed 98\%, and the entire performance range spans less than one percentage point per dataset.}}{5}{}\protected@file@percent }
\newlabel{fig:accuracy}{{4}{5}{}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-B}}Training Dynamics}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Training dynamics on EuroSAT. Top: validation loss. Bottom: validation accuracy. Each line represents one architecture.}}{6}{}\protected@file@percent }
\newlabel{fig:curves_eurosat}{{5}{6}{}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-C}}Per-Class Analysis}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {IV-C}1}EuroSAT}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {IV-C}2}UC Merced}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-D}}Statistical Significance}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Training dynamics on UC Merced. Top: validation loss. Bottom: validation accuracy. Convergence is faster due to the smaller dataset.}}{6}{}\protected@file@percent }
\newlabel{fig:curves_ucmerced}{{6}{6}{}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Per-class F1-score heatmap on EuroSAT. Rows are models, columns are classes. Darker blue indicates higher F1. PermanentCrop and Pasture show the most variation across models.}}{6}{}\protected@file@percent }
\newlabel{fig:f1_eurosat}{{7}{6}{}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Per-class F1-score heatmap on UC Merced (21 classes). Most cells are at F1 = 1.0 (dark blue). Residential sub-types show the most variation.}}{7}{}\protected@file@percent }
\newlabel{fig:f1_ucmerced}{{8}{7}{}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces McNemar's test p-value matrix for EuroSAT. Cells shaded green indicate model pairs whose accuracy difference is not statistically significant ($p > 0.05$); red cells denote pairs where the null hypothesis of equal error rate is rejected ($p < 0.05$). Significance levels: * $p < 0.05$, ** $p < 0.01$.}}{7}{}\protected@file@percent }
\newlabel{fig:mcnemar}{{9}{7}{}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces McNemar's test p-value matrix for UC Merced. Same color scheme as Fig.~\ref {fig:mcnemar}. Pairs involving ViT-B/16 show significant differences; all other models are statistically indistinguishable at $p > 0.05$.}}{7}{}\protected@file@percent }
\newlabel{fig:mcnemar_ucmerced}{{10}{7}{}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-E}}Error Analysis}{7}{}\protected@file@percent }
\citation{liu2022convnet}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Error analysis for ConvNeXt-Tiny on EuroSAT. Top: per-class correct (green) and misclassified (red) counts. Bottom: the most common misclassification patterns.}}{8}{}\protected@file@percent }
\newlabel{fig:prediction}{{11}{8}{}{figure.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-F}}Computational Efficiency}{8}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Training time comparison (seconds). Models sorted by EuroSAT time.}}{8}{}\protected@file@percent }
\newlabel{tab:efficiency}{{IV}{8}{}{table.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion}{8}{}\protected@file@percent }
\newlabel{sec:discussion}{{V}{8}{}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-A}}Architecture Family Comparison}{8}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Misclassified examples from EuroSAT (left) and UC Merced (right). In each pair, the red-bordered image was misclassified and the blue-bordered image is a correctly classified example from the predicted class. The visual similarity within each pair explains the confusion. On UC Merced, residential density classes are the main source of errors.}}{9}{}\protected@file@percent }
\newlabel{fig:misclassified}{{12}{9}{}{figure.12}{}}
\citation{cheng2020remote,cheng2017nwpu}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Accuracy vs.\ parameter count on EuroSAT. Blue circles = classical CNNs, red squares = transformers, green diamonds = modernized CNN.}}{10}{}\protected@file@percent }
\newlabel{fig:efficiency}{{13}{10}{}{figure.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-B}}Deeper Is Not Always Better}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-C}}Parameter Efficiency}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-D}}Dataset Saturation}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-E}}Why Architecture Mattered So Little}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-F}}Limitations}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{10}{}\protected@file@percent }
\bibstyle{IEEEtran}
\bibdata{references}
\bibcite{cheng2020remote}{1}
\bibcite{ma2019deep}{2}
\bibcite{yang2010bag}{3}
\bibcite{cheng2017nwpu}{4}
\bibcite{simonyan2015very}{5}
\bibcite{he2016deep}{6}
\bibcite{huang2017densely}{7}
\bibcite{deng2009imagenet}{8}
\bibcite{nogueira2017towards}{9}
\bibcite{li2018deep}{10}
\bibcite{pan2010survey}{11}
\bibcite{neumann2019domain}{12}
\bibcite{vaswani2017attention}{13}
\bibcite{dosovitskiy2021image}{14}
\bibcite{liu2021swin}{15}
\bibcite{hong2022spectralformer}{16}
\bibcite{wang2023advancing}{17}
\bibcite{liu2022convnet}{18}
\bibcite{mcnemar1947note}{19}
\bibcite{tan2019efficientnet}{20}
\bibcite{zhu2017deep}{21}
\bibcite{helber2019eurosat}{22}
\bibcite{xia2017aid}{23}
\bibcite{wightman2019timm}{24}
\bibcite{loshchilov2019adamw}{25}
\bibcite{paszke2019pytorch}{26}
\bibcite{cohen1960coefficient}{27}
\bibcite{congalton1991review}{28}
\bibcite{foody2002status}{29}
\bibcite{dietterich1998approximate}{30}
\@writefile{toc}{\contentsline {section}{References}{11}{}\protected@file@percent }
\gdef \@abspage@last{11}

{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO3XNCj5Nk5PlAkTeIaPv0W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"RofDUxIWg-N6","executionInfo":{"status":"ok","timestamp":1737346958229,"user_tz":-420,"elapsed":362193,"user":{"displayName":"Akhiyar Waladi","userId":"05156648969656409575"}},"outputId":"47939239-3f2f-4cf2-9499-7e3d87700637"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rasterio in /usr/local/lib/python3.11/dist-packages (1.4.3)\n","Requirement already satisfied: affine in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.4.0)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (24.3.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2024.12.14)\n","Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.1.8)\n","Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.11/dist-packages (from rasterio) (0.7.2)\n","Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (1.26.4)\n","Requirement already satisfied: click-plugins in /usr/local/lib/python3.11/dist-packages (from rasterio) (1.1.1)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.1)\n","Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.11/dist-packages (3.2.0)\n","Mounting Google Drive...\n","Mounted at /content/drive\n","\n","Loading and preprocessing data...\n","Sentinel-2 shape: (10, 2366, 1715)\n","Added indices: NDVI, NDWI, SAVI\n","Final data shape: (4057690, 13)\n","\n","Cleaning plots folder...\n","Folder 'plots' telah dibersihkan dan dibuat ulang\n","\n","Training classifiers...\n","\n","Training Random Forest...\n","Random Forest Training Time: 72.70 seconds\n","\n","Classification Report for Random Forest:\n","              precision    recall  f1-score   support\n","\n","           0       0.78      0.59      0.67    157991\n","           1       0.91      0.95      0.93   3028043\n","           2       0.30      0.03      0.05     49520\n","           3       0.24      0.09      0.13     14570\n","           4       0.37      0.20      0.26    101705\n","           5       0.40      0.50      0.44    157845\n","           6       0.79      0.75      0.77    476197\n","           7       0.60      0.52      0.56     30737\n","           8       0.20      0.00      0.01       506\n","\n","    accuracy                           0.86   4017114\n","   macro avg       0.51      0.40      0.43   4017114\n","weighted avg       0.85      0.86      0.85   4017114\n","\n","\n","Training Extra Trees...\n","Extra Trees Training Time: 72.12 seconds\n","\n","Classification Report for Extra Trees:\n","              precision    recall  f1-score   support\n","\n","           0       0.77      0.59      0.67    157991\n","           1       0.94      0.91      0.92   3028043\n","           2       0.14      0.18      0.16     49520\n","           3       0.13      0.23      0.17     14570\n","           4       0.33      0.26      0.29    101705\n","           5       0.35      0.58      0.43    157845\n","           6       0.72      0.77      0.75    476197\n","           7       0.57      0.56      0.56     30737\n","           8       0.17      0.03      0.06       506\n","\n","    accuracy                           0.83   4017114\n","   macro avg       0.46      0.46      0.45   4017114\n","weighted avg       0.85      0.83      0.84   4017114\n","\n","\n","Training Logistic Regression...\n","Logistic Regression Training Time: 10.88 seconds\n","\n","Classification Report for Logistic Regression:\n","              precision    recall  f1-score   support\n","\n","           0       0.44      0.52      0.48    157991\n","           1       0.98      0.76      0.86   3028043\n","           2       0.07      0.53      0.13     49520\n","           3       0.05      0.65      0.10     14570\n","           4       0.16      0.32      0.22    101705\n","           5       0.37      0.45      0.40    157845\n","           6       0.81      0.73      0.77    476197\n","           7       0.33      0.82      0.47     30737\n","           8       0.01      0.31      0.01       506\n","\n","    accuracy                           0.72   4017114\n","   macro avg       0.36      0.57      0.38   4017114\n","weighted avg       0.87      0.72      0.78   4017114\n","\n","\n","Generating comparison plots...\n","\n","Generating plots for Random Forest...\n","\n","Generating plots for Extra Trees...\n","\n","Generating plots for Logistic Regression...\n","\n","All processing complete! Results saved in 'plots' directory\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x800 with 0 Axes>"]},"metadata":{}}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"Land Cover Classification using Sentinel-2 and Dynamic World Data\"\"\"\n","!pip install rasterio\n","!pip install xlsxwriter\n","# Import required libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","import rasterio\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","import seaborn as sns\n","import pandas as pd\n","from rasterio.enums import Resampling\n","import os\n","import shutil\n","import time\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Constants\n","CLASS_NAMES = {\n","    0: 'Water', 1: 'Trees', 2: 'Grass',\n","    3: 'Flooded vegetation', 4: 'Crops',\n","    5: 'Shrub and scrub', 6: 'Built area',\n","    7: 'Bare ground', 8: 'Snow and ice'\n","}\n","\n","COLORS = ['#419BDF', '#397D49', '#88B053', '#7A87C6',\n","          '#E49635', '#DFC35A', '#C4281B', '#A59B8F', '#B39FE1']\n","\n","FEATURE_NAMES = [\n","    'B2 (Blue)', 'B3 (Green)', 'B4 (Red)',\n","    'B5 (Red Edge 1)', 'B6 (Red Edge 2)', 'B7 (Red Edge 3)',\n","    'B8 (NIR)', 'B8A (Red Edge 4)', 'B11 (SWIR 1)', 'B12 (SWIR 2)',\n","    'NDVI', 'NDWI', 'SAVI'\n","]\n","\n","def calculate_indices(data):\n","    \"\"\"Calculate vegetation indices from Sentinel-2 bands.\"\"\"\n","    height, width = data.shape[1], data.shape[2]\n","\n","    nir = data[6].reshape(height, width)  # NIR band\n","    red = data[2].reshape(height, width)  # Red band\n","    green = data[1].reshape(height, width)  # Green band\n","\n","    epsilon = 1e-10\n","\n","    # Calculate indices\n","    denom = nir + red + epsilon\n","    ndvi = (nir - red) / denom\n","\n","    denom = green + nir + epsilon\n","    ndwi = (green - nir) / denom\n","\n","    denom = nir + red + 0.5 + epsilon\n","    savi = (1.5 * (nir - red)) / denom\n","\n","    # Stack indices\n","    indices = np.stack([ndvi, ndwi, savi])\n","    return indices\n","\n","def load_and_preprocess_data(s2_path, dw_path):\n","    \"\"\"Load and preprocess satellite data.\"\"\"\n","    with rasterio.open(s2_path) as src:\n","        s2_data = src.read()\n","        height, width = src.height, src.width\n","        print(f\"Sentinel-2 shape: {s2_data.shape}\")\n","\n","        indices = calculate_indices(s2_data)\n","        print(\"Added indices: NDVI, NDWI, SAVI\")\n","\n","        s2_data = np.vstack((s2_data, indices))\n","\n","    with rasterio.open(dw_path) as src:\n","        dw_data = src.read(\n","            out_shape=(1, height, width),\n","            resampling=Resampling.nearest\n","        )\n","\n","    return s2_data, dw_data\n","\n","def prepare_data(X, y):\n","    \"\"\"Prepare data for model training.\"\"\"\n","    X = X.reshape(X.shape[0], -1).T\n","    y = y.flatten()\n","\n","    valid_pixels = ~np.isnan(y)\n","    X = X[valid_pixels]\n","    y = y[valid_pixels]\n","\n","    return X, y\n","\n","def create_visualization(plt_func):\n","    \"\"\"Decorator for consistent plot styling.\"\"\"\n","    def wrapper(*args, **kwargs):\n","        plt.figure(figsize=(12, 8))\n","        plt_func(*args, **kwargs)\n","        plt.tight_layout()\n","        if 'save_path' in kwargs:\n","            plt.savefig(kwargs['save_path'], dpi=300, bbox_inches='tight')\n","        plt.close()\n","    return wrapper\n","\n","# Visualization functions\n","@create_visualization\n","def plot_ground_truth(dw_data, save_path=None):\n","    \"\"\"Plot ground truth map.\"\"\"\n","    custom_cmap = plt.matplotlib.colors.ListedColormap(COLORS)\n","    im = plt.imshow(dw_data[0], cmap=custom_cmap, vmin=0, vmax=8)\n","\n","    cbar = plt.colorbar(im, ticks=range(9))\n","    cbar.ax.set_yticklabels([CLASS_NAMES[i] for i in range(9)])\n","\n","    plt.title('Ground Truth Land Cover Classification')\n","    plt.axis('off')\n","\n","    plt.text(0.02, 0.05, '20m/pixel', transform=plt.gca().transAxes,\n","             bbox=dict(facecolor='white', alpha=0.7))\n","    plt.annotate('N↑', xy=(0.02, 0.95), xycoords='axes fraction',\n","                fontsize=12, bbox=dict(facecolor='white', alpha=0.7))\n","\n","@create_visualization\n","def plot_predictions(pipeline, X_all, dw_data, save_path=None):\n","    \"\"\"Plot prediction map.\"\"\"\n","    custom_cmap = plt.matplotlib.colors.ListedColormap(COLORS)\n","    predictions = pipeline.predict(X_all)\n","    prediction_map = predictions.reshape(dw_data[0].shape)\n","\n","    im = plt.imshow(prediction_map, cmap=custom_cmap, vmin=0, vmax=8)\n","    cbar = plt.colorbar(im, ticks=range(9))\n","    cbar.ax.set_yticklabels([CLASS_NAMES[i] for i in range(9)])\n","\n","    plt.title('Classification Results')\n","    plt.axis('off')\n","\n","    plt.text(0.02, 0.05, '20m/pixel', transform=plt.gca().transAxes,\n","             bbox=dict(facecolor='white', alpha=0.7))\n","    plt.annotate('N↑', xy=(0.02, 0.95), xycoords='axes fraction',\n","                fontsize=12, bbox=dict(facecolor='white', alpha=0.7))\n","\n","@create_visualization\n","def plot_confusion_matrix(y_test, y_pred, save_path=None):\n","    \"\"\"Plot confusion matrix.\"\"\"\n","    cm = confusion_matrix(y_test, y_pred)\n","    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n","\n","    sns.heatmap(cm, annot=np.array([[f'{val:,}\\n({percent:.1f}%)'\n","                for val, percent in zip(row, row_percent)]\n","                for row, row_percent in zip(cm, cm_percent)]),\n","                fmt='', cmap='YlOrRd',\n","                xticklabels=[CLASS_NAMES[i] for i in range(9)],\n","                yticklabels=[CLASS_NAMES[i] for i in range(9)])\n","\n","    plt.title('Confusion Matrix')\n","    plt.ylabel('True Label')\n","    plt.xlabel('Predicted Label')\n","    plt.xticks(rotation=45, ha='right')\n","\n","@create_visualization\n","def plot_feature_importance(model, save_path=None):\n","    \"\"\"Plot feature importance.\"\"\"\n","    importance_df = pd.DataFrame({\n","        'feature': FEATURE_NAMES,\n","        'importance': model.feature_importances_\n","    }).sort_values('importance', ascending=True)\n","\n","    plt.barh(range(len(importance_df)), importance_df['importance'], color='#2C7BB6')\n","    plt.yticks(range(len(importance_df)), importance_df['feature'])\n","    plt.xlabel('Relative Importance')\n","    plt.title('Feature Importance in Classification')\n","\n","    for i, v in enumerate(importance_df['importance']):\n","        plt.text(v, i, f' {v:.3f}', va='center')\n","\n","    plt.grid(axis='x', linestyle='--', alpha=0.7)\n","\n","@create_visualization\n","def plot_accuracy_by_class(y_test, y_pred, save_path=None):\n","    \"\"\"Plot accuracy and sample size by class.\"\"\"\n","    metrics = {i: {\n","        'accuracy': (y_pred[y_test == i] == i).mean(),\n","        'samples': np.sum(y_test == i)\n","    } for i in range(9)}\n","\n","    ax1 = plt.gca()\n","    classes = list(metrics.keys())\n","    accuracies = [metrics[c]['accuracy'] for c in classes]\n","    samples = [metrics[c]['samples'] for c in classes]\n","\n","    ax1.bar(classes, accuracies, color='#2C7BB6')\n","    ax1.set_ylim(0, 1)\n","    ax1.set_ylabel('Classification Accuracy')\n","\n","    ax2 = ax1.twinx()\n","    ax2.plot(classes, samples, 'r-o')\n","    ax2.set_ylabel('Number of Samples')\n","\n","    plt.title('Accuracy and Sample Size by Class')\n","    ax1.set_xticks(classes)\n","    ax1.set_xticklabels([CLASS_NAMES[i] for i in classes], rotation=45, ha='right')\n","\n","    for i, (acc, n) in enumerate(zip(accuracies, samples)):\n","        ax1.text(i, acc, f'{acc:.1%}', ha='center', va='bottom')\n","        ax2.text(i, n, f'{n:,}', ha='center', va='top', color='red')\n","\n","    ax1.legend(['Accuracy'], loc='upper left')\n","    ax2.legend(['Sample size'], loc='upper right')\n","    plt.grid(True, alpha=0.3)\n","\n","@create_visualization\n","def plot_classifier_comparison(results, save_path=None):\n","    \"\"\"Plot perbandingan performa classifier.\"\"\"\n","    # Extract metrics\n","    metrics = {}\n","    for name, result in results.items():\n","        report = classification_report(\n","            result['y_test'],\n","            result['y_pred'],\n","            output_dict=True,\n","            zero_division=0\n","        )\n","        metrics[name] = {\n","            'Accuracy': report['accuracy'],\n","            'Macro F1': report['macro avg']['f1-score'],\n","            'Weighted F1': report['weighted avg']['f1-score'],\n","            'Training Time': result['training_time']\n","        }\n","\n","    # Convert to DataFrame\n","    df = pd.DataFrame(metrics).T\n","\n","    # Create subplots\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n","\n","    # Performance metrics\n","    df[['Accuracy', 'Macro F1', 'Weighted F1']].plot(\n","        kind='bar', ax=ax1, width=0.8\n","    )\n","    ax1.set_title('Performance Metrics by Classifier')\n","    ax1.set_xlabel('Classifier')\n","    ax1.set_ylabel('Score')\n","    ax1.grid(True, alpha=0.3)\n","    ax1.legend(title='Metric')\n","\n","    # Training time\n","    df['Training Time'].plot(kind='bar', ax=ax2, color='green', width=0.6)\n","    ax2.set_title('Training Time by Classifier')\n","    ax2.set_xlabel('Classifier')\n","    ax2.set_ylabel('Time (seconds)')\n","    ax2.grid(True, alpha=0.3)\n","\n","    # Rotate x-labels\n","    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n","    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n","\n","    # Add value labels\n","    for ax in [ax1, ax2]:\n","        for container in ax.containers:\n","            ax.bar_label(container, fmt='%.3f', padding=3)\n","\n","    plt.tight_layout()\n","\n","def clean_plots_folder():\n","    \"\"\"Hapus dan buat ulang folder plots.\"\"\"\n","    if os.path.exists('plots'):\n","        shutil.rmtree('plots')\n","    os.makedirs('plots')\n","    print(\"Folder 'plots' telah dibersihkan dan dibuat ulang\")\n","\n","def train_multiple_classifiers(X, y, test_size=0.9):\n","    \"\"\"Train dan evaluasi beberapa classifier.\"\"\"\n","    # Split data\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=test_size, random_state=42, stratify=y\n","    )\n","\n","    # Define preprocessing pipelines\n","    rf_pipeline = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n","        ('scaler', StandardScaler()),\n","        ('classifier', RandomForestClassifier(\n","            n_estimators=100, max_depth=20,\n","            class_weight='balanced', n_jobs=-1, random_state=42))\n","    ])\n","\n","    et_pipeline = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n","        ('scaler', StandardScaler()),\n","        ('classifier', ExtraTreesClassifier(\n","            n_estimators=100, max_depth=20,\n","            class_weight='balanced', n_jobs=-1, random_state=42))\n","    ])\n","\n","    lr_pipeline = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n","        ('scaler', StandardScaler()),\n","        ('classifier', LogisticRegression(\n","            multi_class='multinomial', max_iter=300,\n","            class_weight='balanced', n_jobs=-1, random_state=42))\n","    ])\n","\n","    # Define classifiers with pipelines\n","    classifiers = {\n","        'Random Forest': rf_pipeline,\n","        'Extra Trees': et_pipeline,\n","        'Logistic Regression': lr_pipeline\n","    }\n","\n","    results = {}\n","    for name, pipeline in classifiers.items():\n","        print(f\"\\nTraining {name}...\")\n","        start_time = time.time()\n","\n","        # Train classifier\n","        pipeline.fit(X_train, y_train)\n","\n","        # Make predictions\n","        y_pred = pipeline.predict(X_test)\n","\n","        # Calculate training time\n","        training_time = time.time() - start_time\n","\n","        # Store results\n","        results[name] = {\n","            'classifier': pipeline,\n","            'y_test': y_test,\n","            'y_pred': y_pred,\n","            'training_time': training_time,\n","            'report': classification_report(y_test, y_pred, zero_division=0)\n","        }\n","\n","        print(f\"{name} Training Time: {training_time:.2f} seconds\")\n","        print(f\"\\nClassification Report for {name}:\")\n","        print(results[name]['report'])\n","\n","    return results\n","\n","# Main execution\n","print(\"Mounting Google Drive...\")\n","drive.mount('/content/drive', force_remount=True)\n","\n","s2_path = '/content/drive/My Drive/GEE_Exports/sentinel2_jambi_2023_allbands.tif'\n","dw_path = '/content/drive/My Drive/GEE_Exports/dynamicworld_jambi_2023.tif'\n","\n","# Load and process data\n","print(\"\\nLoading and preprocessing data...\")\n","s2_data, dw_data = load_and_preprocess_data(s2_path, dw_path)\n","X, y = prepare_data(s2_data, dw_data)\n","print(f\"Final data shape: {X.shape}\")\n","\n","# Clean plots folder\n","print(\"\\nCleaning plots folder...\")\n","clean_plots_folder()\n","\n","# Train multiple classifiers\n","print(\"\\nTraining classifiers...\")\n","classifier_results = train_multiple_classifiers(X, y, test_size=0.99)\n","\n","# Generate comparison plot\n","print(\"\\nGenerating comparison plots...\")\n","plot_classifier_comparison(classifier_results, save_path='plots/classifier_comparison.png')\n","\n","# Prepare data for full prediction\n","X_all = s2_data.reshape(s2_data.shape[0], -1).T\n","\n","# Generate individual plots for each classifier\n","for name, result in classifier_results.items():\n","    print(f\"\\nGenerating plots for {name}...\")\n","\n","    # Create subfolder for each classifier\n","    classifier_folder = f'plots/{name.lower().replace(\" \", \"_\")}'\n","    os.makedirs(classifier_folder, exist_ok=True)\n","\n","    # Generate plots\n","    plots = [\n","        (plot_ground_truth, (dw_data,), 'ground_truth.png'),\n","        (plot_predictions, (result['classifier'], X_all, dw_data), 'predictions.png'),\n","        (plot_confusion_matrix, (result['y_test'], result['y_pred']), 'confusion_matrix.png'),\n","        (plot_accuracy_by_class, (result['y_test'], result['y_pred']), 'accuracy_by_class.png')\n","    ]\n","\n","    # Add feature importance plot for supported classifiers\n","    if hasattr(result['classifier'].named_steps['classifier'], 'feature_importances_'):\n","        plots.append(\n","            (plot_feature_importance,\n","             (result['classifier'].named_steps['classifier'],),\n","             'feature_importance.png')\n","        )\n","\n","    for plot_func, args, filename in plots:\n","        plot_func(*args, save_path=f'{classifier_folder}/{filename}')\n","\n","print(\"\\nAll processing complete! Results saved in 'plots' directory\")"]},{"cell_type":"code","source":[],"metadata":{"id":"Nf1zMfM-ubPN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1B8WmfyvubSd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n","\n","\n","def shorten_name(name, max_length=31):\n","    \"\"\"Shorten name to be Excel worksheet compatible.\"\"\"\n","    if len(name) <= max_length:\n","        return name\n","\n","    # Create shorter versions of common words\n","    replacements = {\n","        'Random Forest': 'RF',\n","        'Extra Trees': 'ET',\n","        'Logistic Regression': 'LR',\n","        'Performance': 'Perf',\n","        'Statistics': 'Stats',\n","        'Analysis': 'Anal',\n","        'Distribution': 'Dist',\n","        'Classification': 'Class',\n","        'Confusion Matrix': 'CM',\n","        'Parameters': 'Params'\n","    }\n","\n","    result = name\n","    for old, new in replacements.items():\n","        result = result.replace(old, new)\n","\n","    if len(result) > max_length:\n","        result = result[:max_length]\n","\n","    return result\n","\n","def export_analysis_to_excel(results, s2_data, dw_data, save_path='classification_analysis.xlsx'):\n","    \"\"\"Export detailed analysis results to Excel with accurate percentage calculations.\"\"\"\n","\n","    with pd.ExcelWriter(save_path, engine='xlsxwriter') as writer:\n","        workbook = writer.book\n","        decimal_format = workbook.add_format({'num_format': '0.00'})\n","        # Format untuk angka bulat dengan pemisah ribuan\n","        thousand_format = workbook.add_format({'num_format': '#.##0;[Red]-#.##0'})\n","\n","        # 1. Overall Performance\n","        performance_metrics = {}\n","        for name, result in results.items():\n","            report = classification_report(\n","                result['y_test'],\n","                result['y_pred'],\n","                output_dict=True,\n","                zero_division=0\n","            )\n","            performance_metrics[name] = {\n","                'Overall Accuracy': round(float(report['accuracy']) * 100, 2),\n","                'Macro F1-Score': round(float(report['macro avg']['f1-score']) * 100, 2),\n","                'Weighted F1-Score': round(float(report['weighted avg']['f1-score']) * 100, 2),\n","                'Training Time (seconds)': round(result['training_time'], 2),\n","                'Number of Samples': len(result['y_test'])\n","            }\n","\n","        perf_df = pd.DataFrame(performance_metrics).T\n","        perf_df.to_excel(writer, sheet_name='Overall_Perf')\n","\n","        worksheet = writer.sheets['Overall_Perf']\n","        for col in ['Overall Accuracy', 'Macro F1-Score', 'Weighted F1-Score']:\n","            col_idx = perf_df.columns.get_loc(col) + 1\n","            worksheet.set_column(col_idx, col_idx, None, decimal_format)\n","\n","        # 2. Per-Class Performance\n","        for name, result in results.items():\n","            class_metrics = {}\n","            for i in range(9):\n","                mask = result['y_test'] == i\n","\n","                # Multiply by 100 first, then round\n","                accuracy = np.mean(result['y_test'][mask] == result['y_pred'][mask]) * 100\n","                prec = precision_score(result['y_test'], result['y_pred'],\n","                                    labels=[i], average='macro', zero_division=0) * 100\n","                rec = recall_score(result['y_test'], result['y_pred'],\n","                                 labels=[i], average='macro', zero_division=0) * 100\n","                f1 = f1_score(result['y_test'], result['y_pred'],\n","                            labels=[i], average='macro', zero_division=0) * 100\n","\n","                class_metrics[CLASS_NAMES[i]] = {\n","                    'Total Samples': np.sum(mask),\n","                    'Correct Predictions': np.sum(result['y_test'][mask] == result['y_pred'][mask]),\n","                    'Accuracy': round(accuracy, 2),\n","                    'Precision': round(prec, 2),\n","                    'Recall': round(rec, 2),\n","                    'F1-Score': round(f1, 2)\n","                }\n","\n","            sheet_name = shorten_name(f'{name}_Perf')\n","            class_df = pd.DataFrame(class_metrics).T\n","            class_df.to_excel(writer, sheet_name=sheet_name)\n","\n","            worksheet = writer.sheets[sheet_name]\n","            # Format persentase\n","            for col in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:\n","                col_idx = class_df.columns.get_loc(col) + 1\n","                worksheet.set_column(col_idx, col_idx, None, decimal_format)\n","\n","            # Format angka ribuan\n","            for col in ['Total Samples', 'Correct Predictions']:\n","                col_idx = class_df.columns.get_loc(col) + 1\n","                worksheet.set_column(col_idx, col_idx, None, thousand_format)\n","\n","        # 4. Class Distribution\n","        class_counts = []\n","        valid_counts = []\n","\n","        for i in range(9):\n","            # Menghitung total piksel untuk setiap kelas menggunakan count_nonzero\n","            total_count = int(np.count_nonzero(dw_data[0] == i))\n","            # Menghitung piksel valid menggunakan count_nonzero\n","            valid_mask = (~np.isnan(dw_data[0])) & (dw_data[0] == i)\n","            valid_count = int(np.count_nonzero(valid_mask))\n","            print(f\"Class {i}: {CLASS_NAMES[i]} - Total: {total_count}, Valid: {valid_count}\")\n","\n","            class_counts.append(total_count)\n","            valid_counts.append(valid_count)\n","\n","        class_distribution = pd.DataFrame({\n","            'Class': [CLASS_NAMES[i] for i in range(9)],\n","            'Total Pixels': class_counts,\n","            'Valid Pixels': valid_counts\n","        })\n","\n","        # Menghitung persentase\n","        total_valid_pixels = np.sum(valid_counts)\n","        class_distribution['Percentage'] = [(count / total_valid_pixels * 100)\n","                                          for count in valid_counts]\n","        class_distribution['Percentage'] = class_distribution['Percentage'].round(2)\n","\n","        class_distribution.to_excel(writer, sheet_name='Class_Dist')\n","\n","        worksheet = writer.sheets['Class_Dist']\n","        # Format persentase\n","        percentage_col = class_distribution.columns.get_loc('Percentage') + 1\n","        worksheet.set_column(percentage_col, percentage_col, None, decimal_format)\n","\n","        # Format angka ribuan\n","        for col in ['Total Pixels', 'Valid Pixels']:\n","            col_idx = class_distribution.columns.get_loc(col) + 1\n","            worksheet.set_column(col_idx, col_idx, None, thousand_format)\n","\n","        # 5. Confusion Matrix Analysis\n","        for name, result in results.items():\n","            cm = confusion_matrix(result['y_test'], result['y_pred'])\n","            cm_df = pd.DataFrame(cm,\n","                               index=[f'True_{CLASS_NAMES[i]}' for i in range(9)],\n","                               columns=[f'Pred_{CLASS_NAMES[i]}' for i in range(9)])\n","\n","            total_samples = len(result['y_test'])\n","            cm_stats = {}\n","\n","            for i in range(9):\n","                # Multiply first, then round\n","                proportion = (np.sum(cm[i, :]) / total_samples) * 100\n","                pred_rate = (np.sum(cm[:, i]) / total_samples) * 100\n","\n","                cm_stats[CLASS_NAMES[i]] = {\n","                    'True Positives': cm[i, i],\n","                    'False Positives': np.sum(cm[:, i]) - cm[i, i],\n","                    'False Negatives': np.sum(cm[i, :]) - cm[i, i],\n","                    'Proportion': round(proportion, 2),\n","                    'Prediction Rate': round(pred_rate, 2)\n","                }\n","\n","            sheet_name = shorten_name(f'{name}_CM')\n","            pd.DataFrame(cm_df).to_excel(writer, sheet_name=sheet_name)\n","\n","            stats_sheet = shorten_name(f'{name}_Stats')\n","            stats_df = pd.DataFrame(cm_stats).T\n","            stats_df.to_excel(writer, sheet_name=stats_sheet)\n","\n","            worksheet = writer.sheets[stats_sheet]\n","            # Format persentase\n","            for col in ['Proportion', 'Prediction Rate']:\n","                col_idx = stats_df.columns.get_loc(col) + 1\n","                worksheet.set_column(col_idx, col_idx, None, decimal_format)\n","\n","            # Format angka ribuan\n","            for col in ['True Positives', 'False Positives', 'False Negatives']:\n","                col_idx = stats_df.columns.get_loc(col) + 1\n","                worksheet.set_column(col_idx, col_idx, None, thousand_format)\n","\n","        # 8. Vegetation Indices Analysis\n","        vi_stats = {}\n","        for i, vi_name in enumerate(['NDVI', 'NDWI', 'SAVI']):\n","            vi_data = s2_data[-(3-i)].flatten()\n","            valid_data = vi_data[~np.isnan(vi_data)]\n","\n","            for class_id in range(9):\n","                class_mask = dw_data[0].flatten() == class_id\n","                class_data = vi_data[class_mask]\n","                valid_class_data = class_data[~np.isnan(class_data)]\n","\n","                if len(valid_class_data) > 0:\n","                    vi_stats[f'{vi_name}_{CLASS_NAMES[class_id]}'] = {\n","                        'Mean': round(np.mean(valid_class_data), 2),\n","                        'Median': round(np.median(valid_class_data), 2),\n","                        'Std': round(np.std(valid_class_data), 2),\n","                        'Min': round(np.min(valid_class_data), 2),\n","                        'Max': round(np.max(valid_class_data), 2)\n","                    }\n","\n","        pd.DataFrame(vi_stats).T.to_excel(writer, sheet_name='VI_Stats')\n","\n","    print(f\"\\nDetailed analysis exported to {save_path}\")\n","    return save_path\n","\n","# Setelah training\n","print(\"\\nExporting analysis results...\")\n","\n","export_analysis_to_excel(\n","    classifier_results, s2_data, dw_data,\n","    save_path='plots/landcover_analysis1.xlsx'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":261},"id":"pwH6R5JIubVf","executionInfo":{"status":"ok","timestamp":1737352681224,"user_tz":-420,"elapsed":79021,"user":{"displayName":"Akhiyar Waladi","userId":"05156648969656409575"}},"outputId":"cbfcb654-6d04-4088-e031-21a59aa97179"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Exporting analysis results...\n","Class 0: Water - Total: 159587, Valid: 159587\n","Class 1: Trees - Total: 3058629, Valid: 3058629\n","Class 2: Grass - Total: 50020, Valid: 50020\n","Class 3: Flooded vegetation - Total: 14717, Valid: 14717\n","Class 4: Crops - Total: 102732, Valid: 102732\n","Class 5: Shrub and scrub - Total: 159439, Valid: 159439\n","Class 6: Built area - Total: 481007, Valid: 481007\n","Class 7: Bare ground - Total: 31048, Valid: 31048\n","Class 8: Snow and ice - Total: 511, Valid: 511\n","\n","Detailed analysis exported to plots/landcover_analysis1.xlsx\n"]},{"output_type":"execute_result","data":{"text/plain":["'plots/landcover_analysis1.xlsx'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":[],"metadata":{"id":"nMJ8CpXSudMY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WuYzLd0MudJK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5m4zNUhWucEt"},"execution_count":null,"outputs":[]}]}
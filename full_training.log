
================================================================================
TRAINING MULTIPLE MODEL ARCHITECTURES
================================================================================
Start Time: 2026-01-04 23:35:44
================================================================================

üñ•Ô∏è  Device: cuda
üìä Training samples: 100000
üî¢ Models to train: 5
   - ResNet-50: 25.6M params, RESNET family
   - EfficientNet-B3: 12.0M params, EFFICIENTNET family
   - ConvNeXt-Tiny: 28.6M params, CONVNEXT family
   - DenseNet-121: 8.0M params, DENSENET family
   - Inception-V3: 23.8M params, INCEPTION family

--------------------------------------------------------------------------------
STEP 1: LOADING DATA (once, reused for all variants)
--------------------------------------------------------------------------------

1/4 Loading KLHK ground truth...
‚úì Loaded 27,995 polygons

2/4 Loading Sentinel-2 imagery...
‚úì Loaded mosaic: (10, 11268, 18740)

3/4 Calculating spectral indices...
‚úì Total features: 23 bands

üßπ Cleaning NaN/Inf values...
   Found NaN: True, Inf: False
   ‚úì Replaced with 0

4/4 Rasterizing KLHK labels...
‚úì Rasterized labels: (11268, 18740)

üì¶ Preparing training dataset...

üì¶ Extracting patches:
   Patch size: 32x32
   Stride: 16
   Input shape: (23, 11268, 18740)
   Total possible patches: 822,510
   Valid patches: 480,718
   Sampling 100,000 patches...

   ‚ö†Ô∏è  Remapping non-sequential labels for PyTorch:
     0 ‚Üí 0
     1 ‚Üí 1
     4 ‚Üí 2
     5 ‚Üí 3
     6 ‚Üí 4
     7 ‚Üí 5

   Final patches: 100,000
   Patch shape: (23, 32, 32)

   Class distribution (after remapping):
     Class 0: 1,113 (1.1%)
     Class 1: 37,082 (37.1%)
     Class 2: 57,349 (57.3%)
     Class 3: 175 (0.2%)
     Class 4: 2,782 (2.8%)
     Class 5: 1,499 (1.5%)
‚úì Extracted patches: X=(100000, 23, 32, 32), y=(100000,)

üìä Normalizing each channel independently...
   Channel  0: mean=  0.0229, std=  0.0220
   Channel  5: mean=  0.1912, std=  0.1572
   Channel 10: mean=  0.4788, std=  0.3941
   Channel 15: mean= -0.1568, std=  0.1649
   Channel 20: mean=  0.4261, std=  0.3461
   Channel 22: mean=  0.3413, std=  0.2963
‚úì Normalized to X_normalized shape: (100000, 23, 32, 32)

‚úì Train set: 80,000 samples
‚úì Test set: 20,000 samples


################################################################################
# MODEL 1/5: RESNET50
################################################################################

================================================================================
TRAINING ResNet-50 (RESNET)
================================================================================

üìÅ Output:
   Model: models/resnet50_best.pth
   Results: results/models/resnet50/

üì¶ Creating datasets...
‚úì Train batches: 5000
‚úì Test batches: 1250

üèóÔ∏è  Creating ResNet-50...
   Family: resnet
   Pretrained: Yes (ImageNet)
   Input: 23 channels ‚Üí 6 classes
   ‚úì Total parameters: 23,583,046 (23.58M)
   ‚úì Trainable: 23,583,046 (23.58M)
   ‚úì Device: cuda

üöÄ Training for 30 epochs...
Epoch [ 1/30] Train Loss: 0.7370 Acc: 71.73% | Val Loss: 0.6734 Acc: 75.39% | Time: 102.5s ‚úì BEST
Epoch [ 2/30] Train Loss: 0.6633 Acc: 74.74% | Val Loss: 0.6585 Acc: 75.98% | Time: 89.3s ‚úì BEST
Epoch [ 3/30] Train Loss: 0.6424 Acc: 75.70% | Val Loss: 0.6144 Acc: 76.80% | Time: 87.2s ‚úì BEST
Epoch [ 4/30] Train Loss: 0.6256 Acc: 75.99% | Val Loss: 0.7143 Acc: 76.59% | Time: 90.3s 
Epoch [ 5/30] Train Loss: 0.6134 Acc: 76.43% | Val Loss: 0.6099 Acc: 76.71% | Time: 86.8s 
Epoch [ 6/30] Train Loss: 0.6018 Acc: 76.73% | Val Loss: 0.6842 Acc: 76.48% | Time: 87.0s 
Epoch [ 7/30] Train Loss: 0.5982 Acc: 76.95% | Val Loss: 0.5886 Acc: 77.25% | Time: 87.9s ‚úì BEST
Epoch [ 8/30] Train Loss: 0.5903 Acc: 77.17% | Val Loss: 0.5814 Acc: 77.42% | Time: 89.3s ‚úì BEST
Epoch [ 9/30] Train Loss: 0.5915 Acc: 77.29% | Val Loss: 0.8675 Acc: 76.49% | Time: 87.2s 
Epoch [10/30] Train Loss: 0.5866 Acc: 77.50% | Val Loss: 0.8339 Acc: 76.30% | Time: 86.0s 
Epoch [11/30] Train Loss: 0.5802 Acc: 77.64% | Val Loss: 0.5805 Acc: 77.45% | Time: 87.0s ‚úì BEST
Epoch [12/30] Train Loss: 0.5723 Acc: 77.91% | Val Loss: 0.5897 Acc: 77.34% | Time: 88.9s 
Epoch [13/30] Train Loss: 0.5685 Acc: 78.03% | Val Loss: 0.5572 Acc: 78.24% | Time: 87.6s ‚úì BEST
Epoch [14/30] Train Loss: 0.5659 Acc: 78.17% | Val Loss: 0.6736 Acc: 75.00% | Time: 87.8s 
Epoch [15/30] Train Loss: 0.5612 Acc: 78.25% | Val Loss: 0.5999 Acc: 77.11% | Time: 86.3s 
Epoch [16/30] Train Loss: 0.5570 Acc: 78.45% | Val Loss: 0.5596 Acc: 78.25% | Time: 87.4s ‚úì BEST
Epoch [17/30] Train Loss: 0.5583 Acc: 78.53% | Val Loss: 0.5634 Acc: 78.35% | Time: 90.2s ‚úì BEST
Epoch [18/30] Train Loss: 0.5525 Acc: 78.62% | Val Loss: 0.5726 Acc: 77.82% | Time: 85.2s 
Epoch [19/30] Train Loss: 0.5500 Acc: 78.70% | Val Loss: 0.5868 Acc: 78.46% | Time: 88.8s ‚úì BEST
Epoch [20/30] Train Loss: 0.5521 Acc: 78.82% | Val Loss: 0.5588 Acc: 78.67% | Time: 86.8s ‚úì BEST
Epoch [21/30] Train Loss: 0.5433 Acc: 78.92% | Val Loss: 0.5831 Acc: 78.31% | Time: 89.0s 
Epoch [22/30] Train Loss: 0.5413 Acc: 79.08% | Val Loss: 0.5648 Acc: 78.35% | Time: 85.9s 
Epoch [23/30] Train Loss: 0.5400 Acc: 79.22% | Val Loss: 0.5686 Acc: 78.26% | Time: 89.6s 
Epoch [24/30] Train Loss: 0.5357 Acc: 79.21% | Val Loss: 0.5871 Acc: 78.50% | Time: 89.5s 
Epoch [25/30] Train Loss: 0.5328 Acc: 79.40% | Val Loss: 0.5705 Acc: 78.52% | Time: 88.2s 
Epoch [26/30] Train Loss: 0.5291 Acc: 79.59% | Val Loss: 0.9378 Acc: 76.17% | Time: 87.7s 
Epoch [27/30] Train Loss: 0.5295 Acc: 79.61% | Val Loss: 0.5746 Acc: 78.37% | Time: 87.7s 
Epoch [28/30] Train Loss: 0.5259 Acc: 79.77% | Val Loss: 0.5978 Acc: 77.77% | Time: 87.3s 
Epoch [29/30] Train Loss: 0.5231 Acc: 79.85% | Val Loss: 0.5533 Acc: 79.09% | Time: 88.9s ‚úì BEST
Epoch [30/30] Train Loss: 0.5205 Acc: 79.86% | Val Loss: 0.5674 Acc: 78.45% | Time: 87.9s 

üìä Final evaluation (best model from epoch 29)...

‚úÖ Test Accuracy: 79.09%
‚úÖ F1-Score (Macro): 0.5725
‚úÖ F1-Score (Weighted): 0.7778

‚è±Ô∏è  Training Time: 44.3 minutes
üìä Best Epoch: 29
üìä Best Val Acc: 79.09%

‚úÖ RESNET50 COMPLETE!
   Accuracy: 79.09%
   Time: 44.3 minutes


################################################################################
# MODEL 2/5: EFFICIENTNET_B3
################################################################################

================================================================================
TRAINING EfficientNet-B3 (EFFICIENTNET)
================================================================================

üìÅ Output:
   Model: models/efficientnet_b3_best.pth
   Results: results/models/efficientnet_b3/

üì¶ Creating datasets...
‚úì Train batches: 5000
‚úì Test batches: 1250

üèóÔ∏è  Creating EfficientNet-B3...
   Family: efficientnet
   Pretrained: Yes (ImageNet)
   Input: 23 channels ‚Üí 6 classes
   ‚úì Total parameters: 10,712,654 (10.71M)
   ‚úì Trainable: 10,712,654 (10.71M)
   ‚úì Device: cuda

üöÄ Training for 30 epochs...
Epoch [ 1/30] Train Loss: 1.1533 Acc: 61.33% | Val Loss: 0.7767 Acc: 71.45% | Time: 173.6s ‚úì BEST
Epoch [ 2/30] Train Loss: 0.7428 Acc: 71.54% | Val Loss: 0.6789 Acc: 74.72% | Time: 180.3s ‚úì BEST
Epoch [ 3/30] Train Loss: 0.6949 Acc: 73.55% | Val Loss: 0.7164 Acc: 74.63% | Time: 168.5s 
Epoch [ 4/30] Train Loss: 0.6620 Acc: 74.83% | Val Loss: 0.6667 Acc: 75.55% | Time: 174.1s ‚úì BEST
Epoch [ 5/30] Train Loss: 0.6436 Acc: 75.61% | Val Loss: 1.0939 Acc: 64.01% | Time: 180.4s 
Epoch [ 6/30] Train Loss: 0.6319 Acc: 75.87% | Val Loss: 0.6317 Acc: 77.07% | Time: 178.7s ‚úì BEST
Epoch [ 7/30] Train Loss: 0.6199 Acc: 76.34% | Val Loss: 0.5962 Acc: 77.09% | Time: 179.0s ‚úì BEST
Epoch [ 8/30] Train Loss: 0.6106 Acc: 76.70% | Val Loss: 0.6207 Acc: 77.35% | Time: 180.2s ‚úì BEST
Epoch [ 9/30] Train Loss: 0.6041 Acc: 76.94% | Val Loss: 0.6015 Acc: 77.33% | Time: 182.3s 
Epoch [10/30] Train Loss: 0.6031 Acc: 76.89% | Val Loss: 0.7482 Acc: 77.08% | Time: 178.2s 
Epoch [11/30] Train Loss: 0.5983 Acc: 77.17% | Val Loss: 0.7033 Acc: 76.47% | Time: 180.5s 
Epoch [12/30] Train Loss: 0.5910 Acc: 77.31% | Val Loss: 0.6043 Acc: 77.91% | Time: 179.7s ‚úì BEST
Epoch [13/30] Train Loss: 0.5865 Acc: 77.50% | Val Loss: 0.5909 Acc: 77.30% | Time: 181.2s 
Epoch [14/30] Train Loss: 0.5836 Acc: 77.68% | Val Loss: 0.7521 Acc: 76.09% | Time: 179.6s 
Epoch [15/30] Train Loss: 0.5801 Acc: 77.87% | Val Loss: 0.6323 Acc: 76.93% | Time: 178.3s 
Epoch [16/30] Train Loss: 0.5748 Acc: 77.96% | Val Loss: 0.6100 Acc: 77.01% | Time: 181.5s 
Epoch [17/30] Train Loss: 0.5720 Acc: 78.02% | Val Loss: 0.6946 Acc: 76.62% | Time: 178.2s 
Epoch [18/30] Train Loss: 0.5685 Acc: 78.20% | Val Loss: 0.6588 Acc: 65.25% | Time: 180.2s 
Epoch [19/30] Train Loss: 0.5667 Acc: 78.21% | Val Loss: 0.5876 Acc: 77.91% | Time: 169.6s 
Epoch [20/30] Train Loss: 0.5655 Acc: 78.48% | Val Loss: 0.6449 Acc: 77.59% | Time: 173.6s 
Epoch [21/30] Train Loss: 0.5606 Acc: 78.50% | Val Loss: 0.6450 Acc: 77.09% | Time: 158.7s 
Epoch [22/30] Train Loss: 0.5607 Acc: 78.53% | Val Loss: 0.6375 Acc: 76.91% | Time: 160.0s 
Epoch [23/30] Train Loss: 0.5566 Acc: 78.66% | Val Loss: 0.6674 Acc: 77.66% | Time: 160.4s 
Epoch [24/30] Train Loss: 0.5557 Acc: 78.83% | Val Loss: 0.6123 Acc: 77.53% | Time: 156.1s 
Epoch [25/30] Train Loss: 0.5513 Acc: 78.92% | Val Loss: 0.6216 Acc: 77.59% | Time: 165.8s 
Epoch [26/30] Train Loss: 0.5468 Acc: 79.05% | Val Loss: 0.6051 Acc: 77.98% | Time: 162.0s ‚úì BEST
Epoch [27/30] Train Loss: 0.5459 Acc: 79.11% | Val Loss: 0.6743 Acc: 65.66% | Time: 164.9s 
Epoch [28/30] Train Loss: 0.5462 Acc: 79.18% | Val Loss: 0.6353 Acc: 65.49% | Time: 170.2s 
Epoch [29/30] Train Loss: 0.5431 Acc: 79.30% | Val Loss: 0.6151 Acc: 78.33% | Time: 159.7s ‚úì BEST
Epoch [30/30] Train Loss: 0.5399 Acc: 79.41% | Val Loss: 0.6007 Acc: 77.98% | Time: 159.4s 

üìä Final evaluation (best model from epoch 29)...

‚úÖ Test Accuracy: 78.33%
‚úÖ F1-Score (Macro): 0.5342
‚úÖ F1-Score (Weighted): 0.7669

‚è±Ô∏è  Training Time: 86.6 minutes
üìä Best Epoch: 29
üìä Best Val Acc: 78.33%

‚úÖ EFFICIENTNET_B3 COMPLETE!
   Accuracy: 78.33%
   Time: 86.6 minutes


################################################################################
# MODEL 3/5: CONVNEXT_TINY
################################################################################

================================================================================
TRAINING ConvNeXt-Tiny (CONVNEXT)
================================================================================

üìÅ Output:
   Model: models/convnext_tiny_best.pth
   Results: results/models/convnext_tiny/

üì¶ Creating datasets...
‚úì Train batches: 5000
‚úì Test batches: 1250

üèóÔ∏è  Creating ConvNeXt-Tiny...
   Family: convnext
   Pretrained: Yes (ImageNet)
   Input: 23 channels ‚Üí 6 classes
   ‚úì Total parameters: 27,855,462 (27.86M)
   ‚úì Trainable: 27,855,462 (27.86M)
   ‚úì Device: cuda

üöÄ Training for 30 epochs...
Epoch [ 1/30] Train Loss: 0.7026 Acc: 73.16% | Val Loss: 0.6254 Acc: 75.59% | Time: 91.3s ‚úì BEST
Epoch [ 2/30] Train Loss: 0.6303 Acc: 75.83% | Val Loss: 0.6357 Acc: 76.21% | Time: 89.1s ‚úì BEST
Epoch [ 3/30] Train Loss: 0.6089 Acc: 76.50% | Val Loss: 0.5902 Acc: 76.89% | Time: 92.5s ‚úì BEST
Epoch [ 4/30] Train Loss: 0.5960 Acc: 76.97% | Val Loss: 0.5733 Acc: 77.84% | Time: 86.7s ‚úì BEST
Epoch [ 5/30] Train Loss: 0.5847 Acc: 77.29% | Val Loss: 0.5853 Acc: 77.74% | Time: 89.3s 
Epoch [ 6/30] Train Loss: 0.5751 Acc: 77.75% | Val Loss: 0.5694 Acc: 78.14% | Time: 90.0s ‚úì BEST
Epoch [ 7/30] Train Loss: 0.5662 Acc: 78.09% | Val Loss: 0.5670 Acc: 78.02% | Time: 89.1s 
Epoch [ 8/30] Train Loss: 0.5607 Acc: 78.31% | Val Loss: 0.5823 Acc: 77.69% | Time: 87.3s 
Epoch [ 9/30] Train Loss: 0.5548 Acc: 78.56% | Val Loss: 0.5745 Acc: 78.25% | Time: 92.3s ‚úì BEST
Epoch [10/30] Train Loss: 0.5470 Acc: 78.74% | Val Loss: 0.5611 Acc: 78.49% | Time: 89.2s ‚úì BEST
Epoch [11/30] Train Loss: 0.5428 Acc: 78.94% | Val Loss: 0.5659 Acc: 78.34% | Time: 84.3s 
Epoch [12/30] Train Loss: 0.5486 Acc: 78.97% | Val Loss: 0.5695 Acc: 78.42% | Time: 86.2s 
Epoch [13/30] Train Loss: 0.5378 Acc: 79.31% | Val Loss: 0.5773 Acc: 78.14% | Time: 86.6s 
Epoch [14/30] Train Loss: 0.5287 Acc: 79.57% | Val Loss: 0.5590 Acc: 78.72% | Time: 91.4s ‚úì BEST
Epoch [15/30] Train Loss: 0.5216 Acc: 79.83% | Val Loss: 0.5905 Acc: 78.29% | Time: 94.6s 
Epoch [16/30] Train Loss: 0.5154 Acc: 79.98% | Val Loss: 0.5551 Acc: 78.81% | Time: 93.7s ‚úì BEST
Epoch [17/30] Train Loss: 0.5062 Acc: 80.35% | Val Loss: 0.5711 Acc: 78.55% | Time: 92.0s 
Epoch [18/30] Train Loss: 0.5011 Acc: 80.68% | Val Loss: 0.5768 Acc: 78.42% | Time: 88.3s 
Epoch [19/30] Train Loss: 0.4959 Acc: 80.85% | Val Loss: 0.5691 Acc: 78.82% | Time: 92.3s ‚úì BEST
Epoch [20/30] Train Loss: 0.4873 Acc: 81.25% | Val Loss: 0.5751 Acc: 79.14% | Time: 89.1s ‚úì BEST
Epoch [21/30] Train Loss: 0.4794 Acc: 81.55% | Val Loss: 0.6140 Acc: 78.52% | Time: 90.8s 
Epoch [22/30] Train Loss: 0.4744 Acc: 81.78% | Val Loss: 0.6202 Acc: 78.41% | Time: 91.7s 
Epoch [23/30] Train Loss: 0.4675 Acc: 82.10% | Val Loss: 0.6109 Acc: 78.30% | Time: 84.1s 
Epoch [24/30] Train Loss: 0.4631 Acc: 82.26% | Val Loss: 0.6242 Acc: 77.86% | Time: 91.5s 
Epoch [25/30] Train Loss: 0.4556 Acc: 82.59% | Val Loss: 0.6369 Acc: 78.64% | Time: 92.1s 
Epoch [26/30] Train Loss: 0.4504 Acc: 82.84% | Val Loss: 0.6453 Acc: 78.03% | Time: 95.1s 
Epoch [27/30] Train Loss: 0.4451 Acc: 83.09% | Val Loss: 0.6582 Acc: 78.82% | Time: 85.8s 
Epoch [28/30] Train Loss: 0.4417 Acc: 83.22% | Val Loss: 0.6525 Acc: 78.55% | Time: 89.3s 
Epoch [29/30] Train Loss: 0.4359 Acc: 83.57% | Val Loss: 0.6673 Acc: 78.52% | Time: 90.9s 
Epoch [30/30] Train Loss: 0.4277 Acc: 83.90% | Val Loss: 0.7124 Acc: 78.66% | Time: 94.5s 

üìä Final evaluation (best model from epoch 20)...

‚úÖ Test Accuracy: 79.14%
‚úÖ F1-Score (Macro): 0.5469
‚úÖ F1-Score (Weighted): 0.7785

‚è±Ô∏è  Training Time: 45.1 minutes
üìä Best Epoch: 20
üìä Best Val Acc: 79.14%

‚úÖ CONVNEXT_TINY COMPLETE!
   Accuracy: 79.14%
   Time: 45.1 minutes


################################################################################
# MODEL 4/5: DENSENET121
################################################################################

================================================================================
TRAINING DenseNet-121 (DENSENET)
================================================================================

üìÅ Output:
   Model: models/densenet121_best.pth
   Results: results/models/densenet121/

üì¶ Creating datasets...
‚úì Train batches: 5000
‚úì Test batches: 1250

üèóÔ∏è  Creating DenseNet-121...
   Family: densenet
   Pretrained: Yes (ImageNet)
   Input: 23 channels ‚Üí 6 classes
   ‚úì Total parameters: 7,022,726 (7.02M)
   ‚úì Trainable: 7,022,726 (7.02M)
   ‚úì Device: cuda

üöÄ Training for 30 epochs...
Epoch [ 1/30] Train Loss: 0.7266 Acc: 71.79% | Val Loss: 0.6345 Acc: 75.52% | Time: 205.3s ‚úì BEST
Epoch [ 2/30] Train Loss: 0.6587 Acc: 74.64% | Val Loss: 0.6176 Acc: 75.93% | Time: 209.7s ‚úì BEST
Epoch [ 3/30] Train Loss: 0.6328 Acc: 75.60% | Val Loss: 0.6196 Acc: 76.32% | Time: 204.4s ‚úì BEST
Epoch [ 4/30] Train Loss: 0.6174 Acc: 76.14% | Val Loss: 0.5930 Acc: 77.00% | Time: 198.9s ‚úì BEST
Epoch [ 5/30] Train Loss: 0.6082 Acc: 76.55% | Val Loss: 0.5815 Acc: 77.42% | Time: 197.7s ‚úì BEST
Epoch [ 6/30] Train Loss: 0.5970 Acc: 76.84% | Val Loss: 0.5942 Acc: 77.42% | Time: 194.7s 
Epoch [ 7/30] Train Loss: 0.5919 Acc: 77.04% | Val Loss: 0.5729 Acc: 77.44% | Time: 202.2s ‚úì BEST
Epoch [ 8/30] Train Loss: 0.5848 Acc: 77.33% | Val Loss: 0.5690 Acc: 78.06% | Time: 198.8s ‚úì BEST
Epoch [ 9/30] Train Loss: 0.5815 Acc: 77.43% | Val Loss: 0.5637 Acc: 78.17% | Time: 210.4s ‚úì BEST
Epoch [10/30] Train Loss: 0.5768 Acc: 77.61% | Val Loss: 0.5656 Acc: 78.03% | Time: 196.0s 
Epoch [11/30] Train Loss: 0.5737 Acc: 77.71% | Val Loss: 0.5609 Acc: 78.11% | Time: 194.0s 
Epoch [12/30] Train Loss: 0.5676 Acc: 77.98% | Val Loss: 0.5557 Acc: 78.44% | Time: 191.3s ‚úì BEST
Epoch [13/30] Train Loss: 0.5633 Acc: 78.13% | Val Loss: 0.5685 Acc: 78.59% | Time: 203.6s ‚úì BEST
Epoch [14/30] Train Loss: 0.5636 Acc: 78.18% | Val Loss: 0.5533 Acc: 78.64% | Time: 208.2s ‚úì BEST
Epoch [15/30] Train Loss: 0.5573 Acc: 78.40% | Val Loss: 0.5553 Acc: 78.52% | Time: 201.3s 
Epoch [16/30] Train Loss: 0.5551 Acc: 78.48% | Val Loss: 0.5650 Acc: 77.83% | Time: 198.2s 
Epoch [17/30] Train Loss: 0.5520 Acc: 78.61% | Val Loss: 0.5635 Acc: 78.33% | Time: 216.9s 
Epoch [18/30] Train Loss: 0.5497 Acc: 78.74% | Val Loss: 0.5850 Acc: 78.45% | Time: 194.9s 
Epoch [19/30] Train Loss: 0.5451 Acc: 78.82% | Val Loss: 0.5541 Acc: 78.69% | Time: 212.3s ‚úì BEST
Epoch [20/30] Train Loss: 0.5424 Acc: 78.95% | Val Loss: 0.5679 Acc: 78.29% | Time: 200.9s 
Epoch [21/30] Train Loss: 0.5420 Acc: 78.94% | Val Loss: 0.5597 Acc: 78.70% | Time: 206.7s ‚úì BEST
Epoch [22/30] Train Loss: 0.5387 Acc: 79.04% | Val Loss: 0.6049 Acc: 66.15% | Time: 220.2s 
Epoch [23/30] Train Loss: 0.5356 Acc: 79.25% | Val Loss: 0.5830 Acc: 77.95% | Time: 195.4s 
Epoch [24/30] Train Loss: 0.5335 Acc: 79.32% | Val Loss: 0.6072 Acc: 66.03% | Time: 199.9s 
Epoch [25/30] Train Loss: 0.5327 Acc: 79.45% | Val Loss: 0.5520 Acc: 78.84% | Time: 220.0s ‚úì BEST
Epoch [26/30] Train Loss: 0.5307 Acc: 79.46% | Val Loss: 0.5759 Acc: 78.50% | Time: 205.9s 
Epoch [27/30] Train Loss: 0.5272 Acc: 79.65% | Val Loss: 0.6351 Acc: 65.67% | Time: 202.0s 
Epoch [28/30] Train Loss: 0.5244 Acc: 79.64% | Val Loss: 0.8574 Acc: 76.18% | Time: 208.3s 
Epoch [29/30] Train Loss: 0.5214 Acc: 79.84% | Val Loss: 0.6100 Acc: 66.14% | Time: 217.3s 
Epoch [30/30] Train Loss: 0.5185 Acc: 79.91% | Val Loss: 0.5686 Acc: 78.23% | Time: 199.3s 

üìä Final evaluation (best model from epoch 25)...

‚úÖ Test Accuracy: 78.84%
‚úÖ F1-Score (Macro): 0.5390
‚úÖ F1-Score (Weighted): 0.7751

‚è±Ô∏è  Training Time: 102.1 minutes
üìä Best Epoch: 25
üìä Best Val Acc: 78.84%

‚úÖ DENSENET121 COMPLETE!
   Accuracy: 78.84%
   Time: 102.1 minutes


################################################################################
# MODEL 5/5: INCEPTION_V3
################################################################################

================================================================================
TRAINING Inception-V3 (INCEPTION)
================================================================================

üìÅ Output:
   Model: models/inception_v3_best.pth
   Results: results/models/inception_v3/

üì¶ Creating datasets...
‚úì Train batches: 5000
‚úì Test batches: 1250

üèóÔ∏è  Creating Inception-V3...
   Family: inception
   Pretrained: Yes (ImageNet)
   Input: 23 channels ‚Üí 6 classes
Downloading: "https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth" to C:\Users\MyPC PRO/.cache\torch\hub\checkpoints\inception_v3_google-0cc3c7bd.pth
  0%|          | 0.00/104M [00:00<?, ?B/s]  0%|          | 384k/104M [00:00<00:31, 3.48MB/s]  1%|‚ñè         | 1.50M/104M [00:00<00:13, 8.08MB/s]  3%|‚ñé         | 2.75M/104M [00:00<00:10, 9.86MB/s]  4%|‚ñç         | 4.00M/104M [00:00<00:09, 10.7MB/s]  5%|‚ñç         | 5.12M/104M [00:00<00:09, 11.0MB/s]  6%|‚ñå         | 6.38M/104M [00:00<00:09, 11.3MB/s]  7%|‚ñã         | 7.62M/104M [00:00<00:08, 11.5MB/s]  9%|‚ñä         | 8.88M/104M [00:00<00:08, 11.6MB/s] 10%|‚ñâ         | 10.1M/104M [00:00<00:08, 11.7MB/s] 11%|‚ñà         | 11.4M/104M [00:01<00:08, 11.7MB/s] 12%|‚ñà‚ñè        | 12.6M/104M [00:01<00:08, 11.8MB/s] 13%|‚ñà‚ñé        | 13.8M/104M [00:01<00:08, 11.8MB/s] 14%|‚ñà‚ñç        | 15.0M/104M [00:01<00:07, 11.8MB/s] 16%|‚ñà‚ñå        | 16.2M/104M [00:01<00:08, 11.3MB/s] 17%|‚ñà‚ñã        | 17.4M/104M [00:01<00:08, 11.0MB/s] 18%|‚ñà‚ñä        | 18.5M/104M [00:01<00:08, 10.8MB/s] 19%|‚ñà‚ñâ        | 19.6M/104M [00:01<00:08, 10.6MB/s] 20%|‚ñà‚ñâ        | 20.8M/104M [00:01<00:08, 10.5MB/s] 21%|‚ñà‚ñà        | 21.9M/104M [00:02<00:08, 10.4MB/s] 22%|‚ñà‚ñà‚ñè       | 22.9M/104M [00:02<00:08, 10.4MB/s] 23%|‚ñà‚ñà‚ñé       | 23.9M/104M [00:02<00:08, 10.3MB/s] 24%|‚ñà‚ñà‚ñç       | 24.9M/104M [00:02<00:07, 10.4MB/s] 25%|‚ñà‚ñà‚ñç       | 25.9M/104M [00:02<00:07, 10.3MB/s] 26%|‚ñà‚ñà‚ñå       | 26.9M/104M [00:02<00:07, 10.3MB/s] 27%|‚ñà‚ñà‚ñã       | 27.9M/104M [00:02<00:07, 10.3MB/s] 28%|‚ñà‚ñà‚ñä       | 28.9M/104M [00:02<00:07, 10.2MB/s] 29%|‚ñà‚ñà‚ñâ       | 29.9M/104M [00:02<00:07, 10.3MB/s] 30%|‚ñà‚ñà‚ñâ       | 30.9M/104M [00:03<00:07, 10.3MB/s] 31%|‚ñà‚ñà‚ñà       | 31.9M/104M [00:03<00:07, 10.3MB/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 32.9M/104M [00:03<00:07, 10.3MB/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 33.9M/104M [00:03<00:07, 10.3MB/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 34.9M/104M [00:03<00:07, 10.3MB/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 35.9M/104M [00:03<00:06, 10.2MB/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 37.0M/104M [00:03<00:06, 10.3MB/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 38.0M/104M [00:03<00:06, 10.3MB/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 39.0M/104M [00:03<00:06, 10.3MB/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 40.0M/104M [00:03<00:06, 10.3MB/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 41.0M/104M [00:04<00:06, 10.2MB/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 42.0M/104M [00:04<00:06, 10.3MB/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 43.0M/104M [00:04<00:06, 10.2MB/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 44.0M/104M [00:04<00:06, 10.3MB/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 45.0M/104M [00:04<00:06, 10.2MB/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 46.0M/104M [00:04<00:05, 10.2MB/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 47.0M/104M [00:04<00:05, 10.3MB/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 48.0M/104M [00:04<00:05, 10.2MB/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 49.0M/104M [00:04<00:05, 10.3MB/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 50.0M/104M [00:04<00:05, 10.2MB/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 51.0M/104M [00:05<00:05, 10.1MB/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 52.0M/104M [00:05<00:05, 10.1MB/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 53.0M/104M [00:05<00:05, 10.2MB/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 54.0M/104M [00:05<00:05, 10.2MB/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 55.0M/104M [00:05<00:05, 10.2MB/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 56.0M/104M [00:05<00:04, 10.2MB/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 57.0M/104M [00:05<00:04, 10.2MB/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 58.0M/104M [00:05<00:04, 10.2MB/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 59.0M/104M [00:05<00:04, 10.2MB/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 60.0M/104M [00:06<00:04, 10.2MB/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 61.0M/104M [00:06<00:04, 10.2MB/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 62.0M/104M [00:06<00:04, 10.3MB/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 63.0M/104M [00:06<00:04, 10.3MB/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 64.0M/104M [00:06<00:04, 10.3MB/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 65.0M/104M [00:06<00:03, 10.2MB/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 66.0M/104M [00:06<00:03, 10.3MB/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 67.0M/104M [00:06<00:03, 10.2MB/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 68.0M/104M [00:06<00:03, 10.3MB/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 69.0M/104M [00:06<00:03, 10.3MB/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 70.0M/104M [00:07<00:03, 10.2MB/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 71.0M/104M [00:07<00:03, 10.3MB/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 72.0M/104M [00:07<00:03, 10.3MB/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 73.0M/104M [00:07<00:03, 10.3MB/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 74.0M/104M [00:07<00:03, 10.2MB/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 75.0M/104M [00:07<00:02, 10.2MB/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 76.0M/104M [00:07<00:02, 10.2MB/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 77.0M/104M [00:07<00:02, 10.3MB/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 78.0M/104M [00:07<00:02, 10.2MB/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 79.0M/104M [00:07<00:02, 10.2MB/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 80.0M/104M [00:08<00:02, 10.3MB/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 81.0M/104M [00:08<00:02, 10.3MB/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 82.0M/104M [00:08<00:02, 10.3MB/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 83.0M/104M [00:08<00:02, 10.3MB/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 84.0M/104M [00:08<00:02, 10.3MB/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 85.0M/104M [00:08<00:01, 10.2MB/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 86.0M/104M [00:08<00:01, 10.3MB/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 87.0M/104M [00:08<00:01, 10.2MB/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 88.0M/104M [00:08<00:01, 10.3MB/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 89.0M/104M [00:08<00:01, 10.2MB/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 90.0M/104M [00:09<00:01, 10.3MB/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 91.0M/104M [00:09<00:01, 10.3MB/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 92.0M/104M [00:09<00:01, 10.2MB/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 93.0M/104M [00:09<00:01, 10.2MB/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 94.0M/104M [00:09<00:01, 10.3MB/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 95.0M/104M [00:09<00:00, 10.2MB/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 96.0M/104M [00:09<00:00, 10.3MB/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 97.0M/104M [00:09<00:00, 10.2MB/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 98.0M/104M [00:09<00:00, 10.3MB/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 99.0M/104M [00:10<00:00, 10.2MB/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 100M/104M [00:10<00:00, 10.3MB/s]  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 101M/104M [00:10<00:00, 5.74MB/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 102M/104M [00:10<00:00, 6.87MB/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 103M/104M [00:10<00:00, 7.91MB/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 104M/104M [00:10<00:00, 10.2MB/s]
   ‚úì Total parameters: 24,365,932 (24.37M)
   ‚úì Trainable: 24,365,932 (24.37M)
   ‚úì Device: cuda

üöÄ Training for 30 epochs...
Traceback (most recent call last):
  File "C:\Users\MyPC PRO\Documents\LandCover_Research\scripts\train_models.py", line 465, in <module>
    summary = train_model(
              ^^^^^^^^^^^^
  File "C:\Users\MyPC PRO\Documents\LandCover_Research\scripts\train_models.py", line 318, in train_model
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\landcover_jambi\Lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\landcover_jambi\Lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\landcover_jambi\Lib\site-packages\torchvision\models\inception.py", line 166, in forward
    x, aux = self._forward(x)
             ^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\landcover_jambi\Lib\site-packages\torchvision\models\inception.py", line 105, in _forward
    x = self.Conv2d_1a_3x3(x)
        ^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\landcover_jambi\Lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\landcover_jambi\Lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\landcover_jambi\Lib\site-packages\torchvision\models\inception.py", line 405, in forward
    x = self.conv(x)
        ^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\landcover_jambi\Lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\landcover_jambi\Lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\landcover_jambi\Lib\site-packages\torch\nn\modules\conv.py", line 554, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\landcover_jambi\Lib\site-packages\torch\nn\modules\conv.py", line 549, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
RuntimeError: Given groups=1, weight of size [32, 23, 3, 3], expected input[16, 3, 32, 32] to have 23 channels, but got 3 channels instead

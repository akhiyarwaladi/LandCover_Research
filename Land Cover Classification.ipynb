{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMolPFmm9azHF6i7aNtaPiI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"DqkEmY__5kic"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WNH5uJ5Y5lqo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install rasterio\n","!pip install xlsxwriter"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2wH8NTa9hk5","executionInfo":{"status":"ok","timestamp":1748406380496,"user_tz":-420,"elapsed":24976,"user":{"displayName":"Akhiyar Waladi","userId":"05156648969656409575"}},"outputId":"c2442b11-08e9-49be-e8cc-8898428df27b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rasterio\n","  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n","Collecting affine (from rasterio)\n","  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.4.26)\n","Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.2.1)\n","Collecting cligj>=0.5 (from rasterio)\n","  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n","Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n","Collecting click-plugins (from rasterio)\n","  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.3)\n","Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n","Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n","Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n","Installing collected packages: cligj, click-plugins, affine, rasterio\n","Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.4.3\n","Collecting xlsxwriter\n","  Downloading XlsxWriter-3.2.3-py3-none-any.whl.metadata (2.7 kB)\n","Downloading XlsxWriter-3.2.3-py3-none-any.whl (169 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xlsxwriter\n","Successfully installed xlsxwriter-3.2.3\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"-UUp9uA3kCL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"Enhanced Land Cover Classification using Sentinel-2 and Dynamic World Data\"\"\"\n","\n","# Import required libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","import rasterio\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import (\n","    accuracy_score,\n","    precision_score,\n","    recall_score,\n","    f1_score,\n","    classification_report,\n","    confusion_matrix\n",")\n","# Di bagian import, tambahkan:\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.linear_model import LogisticRegression, SGDClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","import lightgbm as lgb\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","import seaborn as sns\n","import pandas as pd\n","from rasterio.enums import Resampling\n","import os\n","import shutil\n","import time\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Constants\n","CLASS_NAMES = {\n","    0: 'Water', 1: 'Trees', 2: 'Grass',\n","    3: 'Flooded vegetation', 4: 'Crops',\n","    5: 'Shrub and scrub', 6: 'Built area',\n","    7: 'Bare ground', 8: 'Snow and ice'\n","}\n","\n","COLORS = ['#419BDF', '#397D49', '#88B053', '#7A87C6',\n","          '#E49635', '#DFC35A', '#C4281B', '#A59B8F', '#B39FE1']\n","\n","# Initial feature names will be updated after calculating indices\n","FEATURE_NAMES = [\n","    'B2 (Blue)', 'B3 (Green)', 'B4 (Red)',\n","    'B5 (Red Edge 1)', 'B6 (Red Edge 2)', 'B7 (Red Edge 3)',\n","    'B8 (NIR)', 'B8A (Red Edge 4)', 'B11 (SWIR 1)', 'B12 (SWIR 2)'\n","]\n","\n","def calculate_enhanced_indices(data):\n","    \"\"\"\n","    Calculate enhanced set of spectral indices for land cover classification.\n","    \"\"\"\n","    # Extract bands\n","    blue = data[0]    # B2 - Blue\n","    green = data[1]   # B3 - Green\n","    red = data[2]     # B4 - Red\n","    nir = data[6]     # B8 - NIR\n","    swir1 = data[8]   # B11 - SWIR 1\n","    swir2 = data[9]   # B12 - SWIR 2\n","    redEdge1 = data[3]  # B5 - Red Edge 1\n","    redEdge2 = data[4]  # B6 - Red Edge 2\n","\n","    epsilon = 1e-10  # Prevent division by zero\n","\n","    # Basic Vegetation Indices\n","    ndvi = (nir - red) / (nir + red + epsilon)\n","    evi = 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blue + 1 + epsilon))\n","    savi = (1.5 * (nir - red)) / (nir + red + 0.5 + epsilon)\n","\n","    # Water Indices\n","    ndwi = (green - nir) / (green + nir + epsilon)\n","    mndwi = (green - swir1) / (green + swir1 + epsilon)\n","\n","    # Built-up and Bare Soil Indices\n","    ndbi = (swir1 - nir) / (swir1 + nir + epsilon)\n","    bsi = ((swir1 + red) - (nir + blue)) / ((swir1 + red) + (nir + blue) + epsilon)\n","\n","    # Vegetation Red Edge Indices\n","    ndre = (nir - redEdge1) / (nir + redEdge1 + epsilon)\n","    cire = (nir / redEdge1) - 1\n","\n","    # Advanced Vegetation Indices\n","    msavi = (2 * nir + 1 - np.sqrt((2 * nir + 1)**2 - 8 * (nir - red))) / 2\n","    gndvi = (nir - green) / (nir + green + epsilon)\n","\n","    # Moisture and Drought Indices\n","    ndmi = (nir - swir1) / (nir + swir1 + epsilon)\n","    nbri = (nir - swir2) / (nir + swir2 + epsilon)\n","\n","    # Stack all indices\n","    indices = np.stack([\n","        ndvi, evi, savi, ndwi, mndwi,\n","        ndbi, bsi, ndre, cire, msavi,\n","        gndvi, ndmi, nbri\n","    ])\n","\n","    return indices\n","\n","def update_feature_names():\n","    \"\"\"Update global FEATURE_NAMES to include new indices.\"\"\"\n","    global FEATURE_NAMES\n","    FEATURE_NAMES = [\n","        'B2 (Blue)', 'B3 (Green)', 'B4 (Red)',\n","        'B5 (Red Edge 1)', 'B6 (Red Edge 2)', 'B7 (Red Edge 3)',\n","        'B8 (NIR)', 'B8A (Red Edge 4)', 'B11 (SWIR 1)', 'B12 (SWIR 2)',\n","        'NDVI', 'EVI', 'SAVI', 'NDWI', 'MNDWI',\n","        'NDBI', 'BSI', 'NDRE', 'CIRE', 'MSAVI',\n","        'GNDVI', 'NDMI', 'NBRI'\n","    ]\n","\n","def load_and_preprocess_data(s2_path, dw_path):\n","    \"\"\"Load and preprocess satellite data with enhanced indices.\"\"\"\n","    with rasterio.open(s2_path) as src:\n","        s2_data = src.read()\n","        height, width = src.height, src.width\n","        print(f\"Sentinel-2 shape: {s2_data.shape}\")\n","\n","        indices = calculate_enhanced_indices(s2_data)\n","        print(f\"Added {indices.shape[0]} spectral indices\")\n","\n","        s2_data = np.vstack((s2_data, indices))\n","\n","    with rasterio.open(dw_path) as src:\n","        dw_data = src.read(\n","            out_shape=(1, height, width),\n","            resampling=Resampling.nearest\n","        )\n","\n","    # Update feature names to include new indices\n","    update_feature_names()\n","\n","    return s2_data, dw_data\n","\n","def prepare_data(X, y):\n","    \"\"\"Prepare data for model training.\"\"\"\n","    X = X.reshape(X.shape[0], -1).T\n","    y = y.flatten()\n","\n","    valid_pixels = ~np.isnan(y)\n","    X = X[valid_pixels]\n","    y = y[valid_pixels]\n","\n","    return X, y\n","\n","def create_visualization(plt_func):\n","    \"\"\"Decorator for consistent plot styling with transparent background.\"\"\"\n","    def wrapper(*args, **kwargs):\n","        plt.figure(figsize=(12, 8), facecolor='none')\n","        plt_func(*args, **kwargs)\n","        plt.tight_layout()\n","        if 'save_path' in kwargs:\n","            plt.savefig(kwargs['save_path'],\n","                       dpi=300,\n","                       bbox_inches='tight',\n","                       transparent=True)\n","        plt.close()\n","    return wrapper\n","\n","@create_visualization\n","def plot_ground_truth(dw_data, save_path=None):\n","    \"\"\"Plot ground truth map without legend.\"\"\"\n","    custom_cmap = plt.matplotlib.colors.ListedColormap(COLORS)\n","    plt.imshow(dw_data[0], cmap=custom_cmap, vmin=0, vmax=8)\n","    plt.title('Ground Truth Land Cover Classification')\n","    plt.axis('off')\n","\n","    plt.text(0.02, 0.05, '20m/pixel', transform=plt.gca().transAxes,\n","             bbox=dict(facecolor='white', alpha=0.7))\n","    plt.annotate('N↑', xy=(0.02, 0.95), xycoords='axes fraction',\n","                fontsize=12, bbox=dict(facecolor='white', alpha=0.7))\n","\n","@create_visualization\n","def plot_predictions(pipeline, X_all, dw_data, save_path=None):\n","    \"\"\"Plot prediction map with classifier name.\"\"\"\n","    custom_cmap = plt.matplotlib.colors.ListedColormap(COLORS)\n","    predictions = pipeline.predict(X_all)\n","    prediction_map = predictions.reshape(dw_data[0].shape)\n","\n","    plt.imshow(prediction_map, cmap=custom_cmap, vmin=0, vmax=8)\n","\n","    # Get classifier name and format title\n","    classifier_name = pipeline.named_steps['classifier'].__class__.__name__\n","    plt.title(classifier_name, fontweight='bold')\n","    plt.axis('off')\n","\n","    plt.text(0.02, 0.05, '20m/pixel', transform=plt.gca().transAxes,\n","             bbox=dict(facecolor='white', alpha=0.7))\n","    plt.annotate('N↑', xy=(0.02, 0.95), xycoords='axes fraction',\n","                fontsize=12, bbox=dict(facecolor='white', alpha=0.7))\n","\n","@create_visualization\n","def plot_legend_horizontal(save_path=None):\n","    \"\"\"Create horizontal legend plot for land cover classes.\"\"\"\n","    fig, ax = plt.subplots(figsize=(15, 1))  # Reduced height since only 1 row\n","\n","    # Create colored patches for legend\n","    patches = [plt.Rectangle((0,0), 1, 1, fc=COLORS[i]) for i in range(9)]\n","\n","    # Create legend with patches\n","    ax.legend(patches,\n","             [CLASS_NAMES[i] for i in range(9)],\n","             loc='center',\n","             ncol=9,  # Changed to 9 to show all classes in one row\n","             bbox_to_anchor=(0.5, 0.5),\n","             frameon=False)\n","\n","    # Remove axes\n","    ax.set_axis_off()\n","\n","\n","\n","\n","@create_visualization\n","def plot_confusion_matrix(y_test, y_pred, save_path=None):\n","    \"\"\"Plot confusion matrix.\"\"\"\n","    cm = confusion_matrix(y_test, y_pred)\n","    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n","\n","    sns.heatmap(cm, annot=np.array([[f'{val:,}\\n({percent:.1f}%)'\n","                for val, percent in zip(row, row_percent)]\n","                for row, row_percent in zip(cm, cm_percent)]),\n","                fmt='', cmap='YlOrRd',\n","                xticklabels=[CLASS_NAMES[i] for i in range(9)],\n","                yticklabels=[CLASS_NAMES[i] for i in range(9)])\n","\n","    plt.title('Confusion Matrix')\n","    plt.ylabel('True Label')\n","    plt.xlabel('Predicted Label')\n","    plt.xticks(rotation=45, ha='right')\n","\n","@create_visualization\n","def plot_feature_importance(model, save_path=None):\n","    \"\"\"Plot feature importance.\"\"\"\n","    importance_df = pd.DataFrame({\n","        'feature': FEATURE_NAMES,\n","        'importance': model.feature_importances_\n","    }).sort_values('importance', ascending=True)\n","\n","    plt.figure(figsize=(12, 10))  # Increased figure size for more features\n","    plt.barh(range(len(importance_df)), importance_df['importance'], color='#2C7BB6')\n","    plt.yticks(range(len(importance_df)), importance_df['feature'])\n","    plt.xlabel('Relative Importance')\n","    plt.title('Feature Importance in Classification')\n","\n","    for i, v in enumerate(importance_df['importance']):\n","        plt.text(v, i, f' {v:.3f}', va='center')\n","\n","    plt.grid(axis='x', linestyle='--', alpha=0.7)\n","\n","@create_visualization\n","def plot_accuracy_by_class(y_test, y_pred, save_path=None):\n","    \"\"\"Plot accuracy and sample size by class.\"\"\"\n","    metrics = {i: {\n","        'accuracy': (y_pred[y_test == i] == i).mean(),\n","        'samples': np.sum(y_test == i)\n","    } for i in range(9)}\n","\n","    ax1 = plt.gca()\n","    classes = list(metrics.keys())\n","    accuracies = [metrics[c]['accuracy'] for c in classes]\n","    samples = [metrics[c]['samples'] for c in classes]\n","\n","    ax1.bar(classes, accuracies, color='#2C7BB6')\n","    ax1.set_ylim(0, 1)\n","    ax1.set_ylabel('Classification Accuracy')\n","\n","    ax2 = ax1.twinx()\n","    ax2.plot(classes, samples, 'r-o')\n","    ax2.set_ylabel('Number of Samples')\n","\n","    plt.title('Accuracy and Sample Size by Class')\n","    ax1.set_xticks(classes)\n","    ax1.set_xticklabels([CLASS_NAMES[i] for i in classes], rotation=45, ha='right')\n","\n","    for i, (acc, n) in enumerate(zip(accuracies, samples)):\n","        ax1.text(i, acc, f'{acc:.1%}', ha='center', va='bottom')\n","        ax2.text(i, n, f'{n:,}', ha='center', va='top', color='red')\n","\n","    ax1.legend(['Accuracy'], loc='upper left')\n","    ax2.legend(['Sample size'], loc='upper right')\n","    plt.grid(True, alpha=0.3)\n","\n","def clean_plots_folder():\n","    \"\"\"Create plots folder if it doesn't exist.\"\"\"\n","    if os.path.exists('plots'):\n","        shutil.rmtree('plots')\n","    os.makedirs('plots')\n","    print(\"Plots folder has been cleaned and recreated\")\n","\n","\n","\n","def train_multiple_classifiers(X, y, test_size=0.2):\n","    \"\"\"Train and evaluate multiple classifiers.\"\"\"\n","    # Split data\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=test_size, random_state=42, stratify=y\n","    )\n","\n","    # Define preprocessing pipelines\n","    rf_pipeline = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n","        ('scaler', StandardScaler()),\n","        ('classifier', RandomForestClassifier(\n","            n_estimators=200,\n","            max_depth=25,\n","            min_samples_split=5,\n","            min_samples_leaf=2,\n","            class_weight='balanced',\n","            n_jobs=-1,\n","            random_state=42))\n","    ])\n","\n","    et_pipeline = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n","        ('scaler', StandardScaler()),\n","        ('classifier', ExtraTreesClassifier(\n","            n_estimators=200,\n","            max_depth=25,\n","            min_samples_split=5,\n","            min_samples_leaf=2,\n","            class_weight='balanced',\n","            n_jobs=-1,\n","            random_state=42))\n","    ])\n","\n","    lr_pipeline = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n","        ('scaler', StandardScaler()),\n","        ('classifier', LogisticRegression(\n","            multi_class='multinomial',\n","            max_iter=500,\n","            class_weight='balanced',\n","            n_jobs=-1,\n","            random_state=42))\n","    ])\n","\n","    # Fast classifier: Decision Tree\n","    dt_pipeline = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n","        ('scaler', StandardScaler()),\n","        ('classifier', DecisionTreeClassifier(\n","            max_depth=15,  # Reduced depth for speed\n","            min_samples_split=5,\n","            min_samples_leaf=2,\n","            class_weight='balanced',\n","            random_state=42))\n","    ])\n","\n","    # Fast classifier: KNN\n","    knn_pipeline = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n","        ('scaler', StandardScaler()),\n","        ('classifier', KNeighborsClassifier(\n","            n_neighbors=5,\n","            weights='distance',\n","            algorithm='auto',\n","            leaf_size=30,\n","            n_jobs=-1))\n","    ])\n","\n","    # Fast classifier: Naive Bayes\n","    nb_pipeline = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n","        ('scaler', StandardScaler()),\n","        ('classifier', GaussianNB())\n","    ])\n","\n","    # Fast classifier: SGD\n","    sgd_pipeline = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n","        ('scaler', StandardScaler()),\n","        ('classifier', SGDClassifier(\n","            loss='modified_huber',\n","            penalty='l2',\n","            alpha=0.0001,\n","            max_iter=1000,\n","            tol=1e-3,\n","            n_jobs=-1,\n","            random_state=42))\n","    ])\n","\n","    # Fast classifier: LightGBM\n","    lgb_pipeline = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n","        ('scaler', StandardScaler()),\n","        ('classifier', lgb.LGBMClassifier(\n","            n_estimators=100,\n","            learning_rate=0.1,\n","            max_depth=15,\n","            num_leaves=31,\n","            n_jobs=-1,\n","            random_state=42))\n","    ])\n","\n","    # Define all classifiers\n","    classifiers = {\n","        # 'Random Forest': rf_pipeline,\n","        # 'Extra Trees': et_pipeline,\n","        'Logistic Regression': lr_pipeline,\n","        # 'Decision Tree': dt_pipeline,\n","        # 'KNN': knn_pipeline,\n","        # 'Naive Bayes': nb_pipeline,\n","        # 'SGD': sgd_pipeline,\n","        # 'LightGBM': lgb_pipeline\n","    }\n","\n","    results = {}\n","    for name, pipeline in classifiers.items():\n","        print(f\"\\nTraining {name}...\")\n","        start_time = time.time()\n","\n","        # Train classifier\n","        pipeline.fit(X_train, y_train)\n","\n","        # Make predictions\n","        y_pred = pipeline.predict(X_test)\n","\n","        # Calculate training time\n","        training_time = time.time() - start_time\n","\n","        # Store results\n","        results[name] = {\n","            'classifier': pipeline,\n","            'y_test': y_test,\n","            'y_pred': y_pred,\n","            'training_time': training_time,\n","            'report': classification_report(y_test, y_pred, zero_division=0)\n","        }\n","\n","        print(f\"{name} Training Time: {training_time:.2f} seconds\")\n","        print(f\"\\nClassification Report for {name}:\")\n","        print(results[name]['report'])\n","\n","    return results\n","\n","@create_visualization\n","def plot_classifier_comparison(results, save_path=None):\n","    \"\"\"Plot classifier performance comparison.\"\"\"\n","    # Extract metrics\n","    metrics = {}\n","    for name, result in results.items():\n","        report = classification_report(\n","            result['y_test'],\n","            result['y_pred'],\n","            output_dict=True,\n","            zero_division=0\n","        )\n","        metrics[name] = {\n","            'Accuracy': report['accuracy'],\n","            'Macro F1': report['macro avg']['f1-score'],\n","            'Weighted F1': report['weighted avg']['f1-score'],\n","            'Training Time': result['training_time']\n","        }\n","\n","    # Convert to DataFrame\n","    df = pd.DataFrame(metrics).T\n","\n","    # Create subplots\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n","\n","    # Performance metrics\n","    df[['Accuracy', 'Macro F1', 'Weighted F1']].plot(\n","        kind='bar', ax=ax1, width=0.8\n","    )\n","    ax1.set_title('Performance Metrics by Classifier')\n","    ax1.set_xlabel('Classifier')\n","    ax1.set_ylabel('Score')\n","    ax1.grid(True, alpha=0.3)\n","    ax1.legend(title='Metric')\n","\n","    # Training time\n","    df['Training Time'].plot(kind='bar', ax=ax2, color='green', width=0.6)\n","    ax2.set_title('Training Time by Classifier')\n","    ax2.set_xlabel('Classifier')\n","    ax2.set_ylabel('Time (seconds)')\n","    ax2.grid(True, alpha=0.3)\n","\n","    # Rotate x-labels\n","    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n","    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n","\n","    # Add value labels\n","    for ax in [ax1, ax2]:\n","        for container in ax.containers:\n","            ax.bar_label(container, fmt='%.3f', padding=3)\n","\n","    plt.tight_layout()\n","\n","\n","\n","def shorten_name(name, max_length=31):\n","    \"\"\"Shorten name to be Excel worksheet compatible.\"\"\"\n","    if len(name) <= max_length:\n","        return name\n","\n","    # Create shorter versions of common words\n","    replacements = {\n","        'Random Forest': 'RF',\n","        'Extra Trees': 'ET',\n","        'Logistic Regression': 'LR',\n","        'Performance': 'Perf',\n","        'Statistics': 'Stats',\n","        'Analysis': 'Anal',\n","        'Distribution': 'Dist',\n","        'Classification': 'Class',\n","        'Confusion Matrix': 'CM',\n","        'Parameters': 'Params'\n","    }\n","\n","    result = name\n","    for old, new in replacements.items():\n","        result = result.replace(old, new)\n","\n","    if len(result) > max_length:\n","        result = result[:max_length]\n","\n","    return result\n","\n","\n","def export_analysis_to_excel(results, s2_data, dw_data, save_path='classification_analysis.xlsx'):\n","    \"\"\"Export detailed analysis results to Excel with comprehensive analysis.\"\"\"\n","\n","    with pd.ExcelWriter(save_path, engine='xlsxwriter') as writer:\n","        workbook = writer.book\n","        decimal_format = workbook.add_format({'num_format': '0.00'})\n","        percent_format = workbook.add_format({'num_format': '0.00%'})\n","        thousand_format = workbook.add_format({'num_format': '#,##0;[Red]-#,##0'})\n","\n","        # 1. Overall Performance Summary\n","        performance_metrics = {}\n","        for name, result in results.items():\n","            report = classification_report(\n","                result['y_test'],\n","                result['y_pred'],\n","                output_dict=True,\n","                zero_division=0\n","            )\n","            performance_metrics[name] = {\n","                'Overall Accuracy': round(float(report['accuracy']) * 100, 2),\n","                'Macro F1-Score': round(float(report['macro avg']['f1-score']) * 100, 2),\n","                'Weighted F1-Score': round(float(report['weighted avg']['f1-score']) * 100, 2),\n","                'Training Time (seconds)': round(result['training_time'], 2),\n","                'Number of Samples': len(result['y_test'])\n","            }\n","\n","        perf_df = pd.DataFrame(performance_metrics).T\n","        perf_df.to_excel(writer, sheet_name='Overall_Perf')\n","\n","        worksheet = writer.sheets['Overall_Perf']\n","        for col in ['Overall Accuracy', 'Macro F1-Score', 'Weighted F1-Score']:\n","            col_idx = perf_df.columns.get_loc(col) + 1\n","            worksheet.set_column(col_idx, col_idx, None, decimal_format)\n","\n","        # 2. Per-Class Performance for each classifier\n","        for name, result in results.items():\n","            class_metrics = {}\n","            for i in range(9):\n","                mask = result['y_test'] == i\n","\n","                accuracy = np.mean(result['y_test'][mask] == result['y_pred'][mask]) * 100\n","                prec = precision_score(result['y_test'], result['y_pred'],\n","                                    labels=[i], average='macro', zero_division=0) * 100\n","                rec = recall_score(result['y_test'], result['y_pred'],\n","                                 labels=[i], average='macro', zero_division=0) * 100\n","                f1 = f1_score(result['y_test'], result['y_pred'],\n","                            labels=[i], average='macro', zero_division=0) * 100\n","\n","                class_metrics[CLASS_NAMES[i]] = {\n","                    'Total Samples': np.sum(mask),\n","                    'Correct Predictions': np.sum(result['y_test'][mask] == result['y_pred'][mask]),\n","                    'Accuracy': round(accuracy, 2),\n","                    'Precision': round(prec, 2),\n","                    'Recall': round(rec, 2),\n","                    'F1-Score': round(f1, 2)\n","                }\n","\n","            sheet_name = shorten_name(f'{name}_Perf')\n","            class_df = pd.DataFrame(class_metrics).T\n","            class_df.to_excel(writer, sheet_name=sheet_name)\n","\n","            worksheet = writer.sheets[sheet_name]\n","            for col in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:\n","                col_idx = class_df.columns.get_loc(col) + 1\n","                worksheet.set_column(col_idx, col_idx, None, decimal_format)\n","\n","            for col in ['Total Samples', 'Correct Predictions']:\n","                col_idx = class_df.columns.get_loc(col) + 1\n","                worksheet.set_column(col_idx, col_idx, None, thousand_format)\n","\n","        # 3. Class Distribution Analysis\n","        class_counts = []\n","        valid_counts = []\n","\n","        for i in range(9):\n","            total_count = int(np.count_nonzero(dw_data[0] == i))\n","            valid_mask = (~np.isnan(dw_data[0])) & (dw_data[0] == i)\n","            valid_count = int(np.count_nonzero(valid_mask))\n","\n","            class_counts.append(total_count)\n","            valid_counts.append(valid_count)\n","\n","        class_distribution = pd.DataFrame({\n","            'Class': [CLASS_NAMES[i] for i in range(9)],\n","            'Total Pixels': class_counts,\n","            'Valid Pixels': valid_counts,\n","            'Invalid Pixels': [t - v for t, v in zip(class_counts, valid_counts)],\n","            'Validity Rate': [v/t if t > 0 else 0 for v, t in zip(valid_counts, class_counts)]\n","        })\n","\n","        total_valid_pixels = np.sum(valid_counts)\n","        class_distribution['Distribution (%)'] = [(count / total_valid_pixels * 100)\n","                                                for count in valid_counts]\n","\n","        class_distribution.to_excel(writer, sheet_name='Class_Dist')\n","\n","        worksheet = writer.sheets['Class_Dist']\n","        for col in ['Distribution (%)', 'Validity Rate']:\n","            col_idx = class_distribution.columns.get_loc(col) + 1\n","            worksheet.set_column(col_idx, col_idx, None, decimal_format)\n","\n","        for col in ['Total Pixels', 'Valid Pixels', 'Invalid Pixels']:\n","            col_idx = class_distribution.columns.get_loc(col) + 1\n","            worksheet.set_column(col_idx, col_idx, None, thousand_format)\n","\n","        # 4. Confusion Matrix Analysis\n","        for name, result in results.items():\n","            # Basic Confusion Matrix\n","            cm = confusion_matrix(result['y_test'], result['y_pred'])\n","            cm_df = pd.DataFrame(cm,\n","                               index=[f'True_{CLASS_NAMES[i]}' for i in range(9)],\n","                               columns=[f'Pred_{CLASS_NAMES[i]}' for i in range(9)])\n","\n","            sheet_name = shorten_name(f'{name}_CM')\n","            cm_df.to_excel(writer, sheet_name=sheet_name)\n","\n","            # Detailed Statistics\n","            total_samples = len(result['y_test'])\n","            cm_stats = {}\n","\n","            for i in range(9):\n","                true_pos = cm[i, i]\n","                false_pos = np.sum(cm[:, i]) - cm[i, i]\n","                false_neg = np.sum(cm[i, :]) - cm[i, i]\n","                true_neg = np.sum(cm) - true_pos - false_pos - false_neg\n","\n","                precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n","                recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n","                specificity = true_neg / (true_neg + false_pos) if (true_neg + false_pos) > 0 else 0\n","                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","\n","                cm_stats[CLASS_NAMES[i]] = {\n","                    'True Positives': true_pos,\n","                    'False Positives': false_pos,\n","                    'False Negatives': false_neg,\n","                    'True Negatives': true_neg,\n","                    'Precision': precision,\n","                    'Recall': recall,\n","                    'Specificity': specificity,\n","                    'F1-Score': f1,\n","                    'Support': np.sum(cm[i, :]),\n","                    'Predictions': np.sum(cm[:, i])\n","                }\n","\n","            stats_sheet = shorten_name(f'{name}_Stats')\n","            stats_df = pd.DataFrame(cm_stats).T\n","            stats_df.to_excel(writer, sheet_name=stats_sheet)\n","\n","            worksheet = writer.sheets[stats_sheet]\n","            for col in ['Precision', 'Recall', 'Specificity', 'F1-Score']:\n","                col_idx = stats_df.columns.get_loc(col) + 1\n","                worksheet.set_column(col_idx, col_idx, None, percent_format)\n","\n","            for col in ['True Positives', 'False Positives', 'False Negatives',\n","                       'True Negatives', 'Support', 'Predictions']:\n","                col_idx = stats_df.columns.get_loc(col) + 1\n","                worksheet.set_column(col_idx, col_idx, None, thousand_format)\n","\n","        # 5. Detailed Vegetation Indices Analysis\n","        vi_stats = {}\n","        vi_names = ['NDVI', 'EVI', 'SAVI', 'NDWI', 'MNDWI', 'NDBI', 'BSI',\n","                    'NDRE', 'CIRE', 'MSAVI', 'GNDVI', 'NDMI', 'NBRI']\n","\n","        for i, vi_name in enumerate(vi_names):\n","            vi_data = s2_data[-(len(vi_names)-i)].flatten()\n","            valid_data = vi_data[~np.isnan(vi_data)]\n","\n","            for class_id in range(9):\n","                class_mask = dw_data[0].flatten() == class_id\n","                class_data = vi_data[class_mask]\n","                valid_class_data = class_data[~np.isnan(class_data)]\n","\n","                if len(valid_class_data) > 0:\n","                    # Calculate additional statistics\n","                    percentiles = np.percentile(valid_class_data, [25, 50, 75])\n","                    vi_stats[f'{vi_name}_{CLASS_NAMES[class_id]}'] = {\n","                        'Mean': np.mean(valid_class_data),\n","                        'Std': np.std(valid_class_data),\n","                        'Min': np.min(valid_class_data),\n","                        'Q1': percentiles[0],\n","                        'Median': percentiles[1],\n","                        'Q3': percentiles[2],\n","                        'Max': np.max(valid_class_data),\n","                        'Range': np.max(valid_class_data) - np.min(valid_class_data),\n","                        'IQR': percentiles[2] - percentiles[0],\n","                        'Samples': len(valid_class_data)\n","                    }\n","\n","        vi_stats_df = pd.DataFrame(vi_stats).T\n","        vi_stats_df.to_excel(writer, sheet_name='VI_Stats')\n","\n","        worksheet = writer.sheets['VI_Stats']\n","        for col in vi_stats_df.columns:\n","            if col != 'Samples':\n","                col_idx = vi_stats_df.columns.get_loc(col) + 1\n","                worksheet.set_column(col_idx, col_idx, None, decimal_format)\n","            else:\n","                col_idx = vi_stats_df.columns.get_loc(col) + 1\n","                worksheet.set_column(col_idx, col_idx, None, thousand_format)\n","\n","        # 6. Band Statistics Analysis\n","        band_stats = {}\n","        for i, band_name in enumerate(FEATURE_NAMES[:10]):  # Original 10 bands\n","            band_data = s2_data[i].flatten()\n","            valid_data = band_data[~np.isnan(band_data)]\n","\n","            for class_id in range(9):\n","                class_mask = dw_data[0].flatten() == class_id\n","                class_data = band_data[class_mask]\n","                valid_class_data = class_data[~np.isnan(class_data)]\n","\n","                if len(valid_class_data) > 0:\n","                    percentiles = np.percentile(valid_class_data, [25, 50, 75])\n","                    band_stats[f'{band_name}_{CLASS_NAMES[class_id]}'] = {\n","                        'Mean': np.mean(valid_class_data),\n","                        'Std': np.std(valid_class_data),\n","                        'Min': np.min(valid_class_data),\n","                        'Q1': percentiles[0],\n","                        'Median': percentiles[1],\n","                        'Q3': percentiles[2],\n","                        'Max': np.max(valid_class_data),\n","                        'Range': np.max(valid_class_data) - np.min(valid_class_data),\n","                        'IQR': percentiles[2] - percentiles[0],\n","                        'Samples': len(valid_class_data)\n","                    }\n","\n","        band_stats_df = pd.DataFrame(band_stats).T\n","        band_stats_df.to_excel(writer, sheet_name='Band_Stats')\n","\n","        worksheet = writer.sheets['Band_Stats']\n","        for col in band_stats_df.columns:\n","            if col != 'Samples':\n","                col_idx = band_stats_df.columns.get_loc(col) + 1\n","                worksheet.set_column(col_idx, col_idx, None, decimal_format)\n","            else:\n","                col_idx = band_stats_df.columns.get_loc(col) + 1\n","                worksheet.set_column(col_idx, col_idx, None, thousand_format)\n","\n","    print(f\"\\nDetailed analysis exported to {save_path}\")\n","    return save_path\n","\n","def enhance_rgb(rgb_bands, percentile=2):\n","    \"\"\"\n","    Meningkatkan kualitas visual dari citra RGB.\n","\n","    Args:\n","        rgb_bands: Array dengan 3 band (R,G,B)\n","        percentile: Persentil untuk pemotongan histogram\n","    Returns:\n","        rgb_enhanced: Array RGB yang telah ditingkatkan kualitasnya\n","    \"\"\"\n","    rgb_enhanced = np.zeros_like(rgb_bands)\n","\n","    for i in range(3):\n","        band = rgb_bands[:,:,i]\n","        p_low, p_high = np.percentile(band[~np.isnan(band)], (percentile, 100-percentile))\n","        band_enhanced = np.clip(band, p_low, p_high)\n","        band_enhanced = (band_enhanced - p_low) / (p_high - p_low)\n","        rgb_enhanced[:,:,i] = band_enhanced\n","\n","    return np.clip(rgb_enhanced, 0, 1)\n","\n","\n","# # Modifikasi konstanta warna untuk Dynamic World dengan skema baru\n","# COLORS = [\n","#     '#1E90FF',  # Water - Biru laut\n","#     '#228B22',  # Trees - Forest green\n","#     '#32CD32',  # Grass - Lime green\n","#     '#6B8E23',  # Flooded vegetation - Olive drab\n","#     '#DAA520',  # Crops - Golden rod\n","#     '#8FBC8F',  # Shrub and scrub - Dark sea green\n","#     '#CD5C5C',  # Built area - Indian red\n","#     '#DEB887',  # Bare ground - Burlywood\n","#     '#F0F8FF'   # Snow and ice - Alice blue\n","# ]\n","\n","def plot_raw_data_comparison(s2_data, dw_data, save_dir='plots/raw_data'):\n","    \"\"\"\n","    Create visualization images showing data before and after preprocessing,\n","    plus other aspects of the data.\n","\n","    Args:\n","        s2_data: Sentinel-2 data array with all bands and indices\n","        dw_data: Dynamic World classification data\n","        save_dir: Directory to save output visualizations\n","    \"\"\"\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    # 1. Raw Sentinel-2 RGB (Simulating before cloud removal and preprocessing)\n","    plt.figure(figsize=(8, 8))\n","\n","    # Create a copy of the RGB data\n","    rgb_raw = np.dstack((\n","        s2_data[2],  # Red (B4)\n","        s2_data[1],  # Green (B3)\n","        s2_data[0]   # Blue (B2)\n","    ))\n","\n","    # Simulate raw data by adding noise, keeping original values, and simulating clouds\n","    rgb_raw = np.nan_to_num(rgb_raw)\n","\n","    # Create a cloud mask (random cloud-like pattern)\n","    cloud_mask = np.zeros_like(rgb_raw[:,:,0], dtype=bool)\n","\n","    # Add some random \"cloud\" patches (for demonstration purposes)\n","    for _ in range(5):  # Add 5 cloud patches\n","        x_center = np.random.randint(50, rgb_raw.shape[0]-50)\n","        y_center = np.random.randint(50, rgb_raw.shape[1]-50)\n","\n","        # Cloud size\n","        size_x = np.random.randint(30, 100)\n","        size_y = np.random.randint(30, 100)\n","\n","        # Create cloud with a smooth gradient\n","        x, y = np.ogrid[-size_x:size_x, -size_y:size_y]\n","        mask = x*x + y*y <= size_x*size_y\n","\n","        # Apply cloud to a region of the cloud mask\n","        x_min = max(0, x_center - size_x)\n","        x_max = min(cloud_mask.shape[0], x_center + size_x)\n","        y_min = max(0, y_center - size_y)\n","        y_max = min(cloud_mask.shape[1], y_center + size_y)\n","\n","        cloud_patch = np.zeros((x_max-x_min, y_max-y_min), dtype=bool)\n","        patch_mask = mask[\n","            max(0, size_x-x_center):min(2*size_x, size_x+cloud_mask.shape[0]-x_center),\n","            max(0, size_y-y_center):min(2*size_y, size_y+cloud_mask.shape[1]-y_center)\n","        ]\n","        cloud_patch[:patch_mask.shape[0], :patch_mask.shape[1]] = patch_mask\n","        cloud_mask[x_min:x_max, y_min:y_max] |= cloud_patch\n","\n","    # Apply clouds to the image (bright white)\n","    rgb_raw_with_clouds = rgb_raw.copy()\n","    rgb_raw_with_clouds[cloud_mask, :] = 1.0  # White clouds\n","\n","    # Add some noise to simulate raw data\n","    noise = np.random.normal(0, 0.05, rgb_raw.shape)\n","    rgb_raw_with_clouds = np.clip(rgb_raw_with_clouds + noise, 0, 1)\n","\n","    # Display the raw image\n","    plt.imshow(rgb_raw_with_clouds)\n","    plt.axis('off')\n","    plt.tight_layout(pad=0)\n","    plt.savefig(os.path.join(save_dir, 'sentinel2_rgb_raw.png'),\n","               dpi=300, bbox_inches='tight', transparent=True)\n","    plt.close()\n","\n","    # 2. Processed Sentinel-2 RGB (After cloud removal and preprocessing)\n","    plt.figure(figsize=(8, 8))\n","\n","    rgb = np.dstack((\n","        s2_data[2],  # Red (B4)\n","        s2_data[1],  # Green (B3)\n","        s2_data[0]   # Blue (B2)\n","    ))\n","\n","    # Normalize and enhance visual quality\n","    rgb = np.nan_to_num(rgb)\n","    rgb_enhanced = enhance_rgb(rgb, percentile=2)\n","    rgb_enhanced = np.clip(rgb_enhanced * 1.3, 0, 1)\n","\n","    plt.imshow(rgb_enhanced)\n","    plt.axis('off')\n","    plt.tight_layout(pad=0)\n","    plt.savefig(os.path.join(save_dir, 'sentinel2_rgb_processed.png'),\n","               dpi=300, bbox_inches='tight', transparent=True)\n","    plt.close()\n","\n","    # 3. False Color Composite\n","    plt.figure(figsize=(8, 8))\n","\n","    false_color = np.dstack((\n","        s2_data[6],  # NIR (B8)\n","        s2_data[2],  # Red (B4)\n","        s2_data[1]   # Green (B3)\n","    ))\n","\n","    false_color = np.nan_to_num(false_color)\n","    false_color_enhanced = enhance_rgb(false_color, percentile=2)\n","\n","    plt.imshow(false_color_enhanced)\n","    plt.axis('off')\n","    plt.tight_layout(pad=0)\n","    plt.savefig(os.path.join(save_dir, 'false_color.png'),\n","               dpi=300, bbox_inches='tight', transparent=True)\n","    plt.close()\n","\n","    # 4. Spectral Indices Composite\n","    plt.figure(figsize=(8, 8))\n","\n","    # Find indices for NDVI, NDWI, NDBI\n","    indices_list = ['NDVI', 'NDWI', 'NDBI']\n","    indices_positions = []\n","\n","    feature_names = FEATURE_NAMES[10:]  # Skip the first 10 which are original bands\n","    for idx_name in indices_list:\n","        if idx_name in feature_names:\n","            # Get position offset by 10 (to account for original bands)\n","            indices_positions.append(feature_names.index(idx_name) + 10)\n","\n","    # If indices not found, use default positions (first indices after bands)\n","    if len(indices_positions) < 3:\n","        indices_positions = [10, 13, 15]  # NDVI, NDWI, NDBI typical positions\n","\n","    # Create RGB composite from 3 indices\n","    indices_composite = np.dstack((\n","        np.clip(s2_data[indices_positions[2]], -1, 1),  # NDBI -> Red\n","        np.clip(s2_data[indices_positions[0]], -1, 1),  # NDVI -> Green\n","        np.clip(s2_data[indices_positions[1]], -1, 1)   # NDWI -> Blue\n","    ))\n","\n","    indices_composite = np.nan_to_num(indices_composite)\n","\n","    # Normalize each index to 0-1 range for visualization\n","    for i in range(3):\n","        min_val = np.percentile(indices_composite[:,:,i], 2)\n","        max_val = np.percentile(indices_composite[:,:,i], 98)\n","        indices_composite[:,:,i] = np.clip((indices_composite[:,:,i] - min_val) / (max_val - min_val), 0, 1)\n","\n","    plt.imshow(indices_composite)\n","    plt.axis('off')\n","    plt.tight_layout(pad=0)\n","    plt.savefig(os.path.join(save_dir, 'spectral_indices.png'),\n","               dpi=300, bbox_inches='tight', transparent=True)\n","    plt.close()\n","\n","    # 5. Dynamic World Classification\n","    plt.figure(figsize=(8, 8))\n","\n","    custom_cmap = plt.matplotlib.colors.ListedColormap(COLORS)\n","    plt.imshow(dw_data[0], cmap=custom_cmap, vmin=0, vmax=8)\n","    plt.axis('off')\n","    plt.tight_layout(pad=0)\n","    plt.savefig(os.path.join(save_dir, 'dw_classification.png'),\n","               dpi=300, bbox_inches='tight', transparent=True)\n","    plt.close()\n","\n","    # 6. Create a color legend only\n","    plt.figure(figsize=(10, 1))\n","    handles = [plt.Rectangle((0,0), 1, 1, color=COLORS[i]) for i in range(9)]\n","    plt.legend(handles, [CLASS_NAMES[i] for i in range(9)],\n","               loc='center', ncol=9, frameon=False)\n","    plt.axis('off')\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(save_dir, 'color_legend.png'),\n","               dpi=300, bbox_inches='tight', transparent=True)\n","    plt.close()\n","\n","    print(f\"Visualization images saved to {save_dir}:\")\n","    print(f\"1. sentinel2_rgb_raw.png - RGB Natural Color (Before Preprocessing)\")\n","    print(f\"2. sentinel2_rgb_processed.png - RGB Natural Color (After Preprocessing)\")\n","    print(f\"3. false_color.png - False Color Composite (NIR-R-G)\")\n","    print(f\"4. spectral_indices.png - Spectral Indices (NDBI-NDVI-NDWI)\")\n","    print(f\"5. dw_classification.png - Dynamic World Classification\")\n","    print(f\"6. color_legend.png - Color Legend for Classes\")\n","\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","\n","    # Mount Google Drive (if using Colab)\n","    print(\"Mounting Google Drive...\")\n","    drive.mount('/content/drive', force_remount=True)\n","\n","    s2_path = '/content/drive/My Drive/GEE_Exports/sentinel2_jambi_2023_allbands.tif'\n","    dw_path = '/content/drive/My Drive/GEE_Exports/dynamicworld_jambi_2023.tif'\n","\n","    # Load and process data\n","    print(\"\\nLoading and preprocessing data...\")\n","    s2_data, dw_data = load_and_preprocess_data(s2_path, dw_path)\n","    print(s2_data)\n","    print(dw_data)\n","    print()\n","    X, y = prepare_data(s2_data, dw_data)\n","    print(f\"Final data shape: {X.shape}\")\n","    print('Data Band')\n","    print(X)\n","    print('Data Label')\n","    print(y)\n","    # Clean plots folder\n","    print(\"\\nCleaning plots folder...\")\n","    clean_plots_folder()\n","\n","    print(\"\\nMembuat visualisasi perbandingan data...\")\n","    plot_raw_data_comparison(s2_data, dw_data)\n","\n","    # # Train multiple classifiers\n","    # print(\"\\nTraining classifiers...\")\n","    # classifier_results = train_multiple_classifiers(X, y, test_size=0.99)\n","\n","    # # Generate comparison plot\n","    # print(\"\\nGenerating comparison plots...\")\n","    # plot_classifier_comparison(classifier_results, save_path='plots/classifier_comparison.png')\n","\n","    # # Prepare data for full prediction\n","    # X_all = s2_data.reshape(s2_data.shape[0], -1).T\n","\n","    # # In the main execution block, where other plots are generated\n","    # for name, result in classifier_results.items():\n","    #     print(f\"\\nGenerating plots for {name}...\")\n","    #     classifier_folder = f'plots/{name.lower().replace(\" \", \"_\")}'\n","    #     os.makedirs(classifier_folder, exist_ok=True)\n","\n","    #     # Generate plots\n","    #     plots = [\n","    #         (plot_ground_truth, (dw_data,), 'ground_truth.png'),\n","    #         (plot_predictions, (result['classifier'], X_all, dw_data), 'predictions.png'),\n","    #         (plot_confusion_matrix, (result['y_test'], result['y_pred']), 'confusion_matrix.png'),\n","    #         (plot_accuracy_by_class, (result['y_test'], result['y_pred']), 'accuracy_by_class.png'),\n","    #         (plot_legend_horizontal, (), 'legend.png')  # Add this line\n","    #     ]\n","\n","    #     # Add feature importance plot for supported classifiers\n","    #     if hasattr(result['classifier'].named_steps['classifier'], 'feature_importances_'):\n","    #         plots.append(\n","    #             (plot_feature_importance,\n","    #              (result['classifier'].named_steps['classifier'],),\n","    #              'feature_importance.png')\n","    #         )\n","\n","    #     for plot_func, args, filename in plots:\n","    #         plot_func(*args, save_path=f'{classifier_folder}/{filename}')\n","\n","    # # Export detailed analysis to Excel\n","    # print(\"\\nExporting analysis results...\")\n","    # export_analysis_to_excel(\n","    #     classifier_results, s2_data, dw_data,\n","    #     save_path='plots/landcover_analysis.xlsx'\n","    # )\n","\n","    print(\"\\nAll processing complete! Results saved in 'plots' directory\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0l1lDn2_XQ9m","executionInfo":{"status":"ok","timestamp":1748407609262,"user_tz":-420,"elapsed":35443,"user":{"displayName":"Akhiyar Waladi","userId":"05156648969656409575"}},"outputId":"838117be-ff87-4dd5-dfca-fc65c80255b9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounting Google Drive...\n","Mounted at /content/drive\n","\n","Loading and preprocessing data...\n","Sentinel-2 shape: (10, 2366, 1715)\n","Added 13 spectral indices\n","[[[ 0.2223      0.2207      0.2162     ...  0.2484      0.1996\n","    0.1864    ]\n","  [ 0.2089      0.2065      0.2        ...  0.2701      0.2566\n","    0.2316    ]\n","  [ 0.185       0.1788      0.18       ...  0.2706      0.2691\n","    0.2661    ]\n","  ...\n","  [ 0.1641      0.1627      0.1589     ...  0.1885      0.18675\n","    0.1841    ]\n","  [ 0.1628      0.1629      0.1586     ...  0.183       0.1787\n","    0.18225   ]\n","  [ 0.1568      0.1619      0.1597     ...  0.17805     0.1772\n","    0.1681    ]]\n","\n"," [[ 0.2475      0.2433      0.2382     ...  0.299       0.2282\n","    0.2083    ]\n","  [ 0.2302      0.2265      0.2227     ...  0.3366      0.316\n","    0.2693    ]\n","  [ 0.2012      0.1972      0.1979     ...  0.3376      0.335\n","    0.3313    ]\n","  ...\n","  [ 0.1752      0.1737      0.1751     ...  0.2019      0.1979\n","    0.19415   ]\n","  [ 0.1738      0.1729      0.1749     ...  0.19345     0.18975\n","    0.1944    ]\n","  [ 0.1665      0.1732      0.1709     ...  0.1896      0.1898\n","    0.1775    ]]\n","\n"," [[ 0.2608      0.254       0.255      ...  0.3277      0.2188\n","    0.1867    ]\n","  [ 0.2346      0.2299      0.2199     ...  0.3848      0.3563\n","    0.2825    ]\n","  [ 0.1927      0.1787      0.173      ...  0.3911      0.3877\n","    0.3799    ]\n","  ...\n","  [ 0.178       0.1672      0.1627     ...  0.22065     0.21955\n","    0.21735   ]\n","  [ 0.1767      0.1706      0.1562     ...  0.2007      0.19715\n","    0.20095   ]\n","  [ 0.1631      0.1663      0.1593     ...  0.1966      0.1975\n","    0.1561    ]]\n","\n"," ...\n","\n"," [[ 0.25260454  0.25687233  0.2557413  ...  0.07587701  0.24374479\n","    0.3288223 ]\n","  [ 0.26862592  0.26687166  0.30698615 ... -0.01385543  0.01680145\n","    0.13100997]\n","  [ 0.30728182  0.3675433   0.38683194 ... -0.01457551 -0.01994215\n","   -0.01579025]\n","  ...\n","  [ 0.26847598  0.30810592  0.37940812 ...  0.29608646  0.29352966\n","    0.2920693 ]\n","  [ 0.2523123   0.26938513  0.35485065 ...  0.31125945  0.32328814\n","    0.33407554]\n","  [ 0.29374337  0.30567253  0.3328128  ...  0.31347877  0.32001793\n","    0.3853878 ]]\n","\n"," [[-0.12609293 -0.1292848  -0.13884722 ...  0.11392002  0.15512462\n","    0.18556847]\n","  [-0.07225834 -0.05777561 -0.00955076 ...  0.09865774  0.10163491\n","    0.12556195]\n","  [ 0.02663241  0.11433425  0.15808539 ...  0.11265693  0.10353104\n","    0.09668604]\n","  ...\n","  [-0.13063388 -0.0299808   0.05732138 ... -0.07352027 -0.08911517\n","   -0.10596694]\n","  [-0.12424789 -0.06794912  0.07162657 ... -0.05666903 -0.02885558\n","   -0.01104619]\n","  [-0.02149501  0.0186083   0.0556586  ... -0.06915577 -0.02642359\n","    0.1446145 ]]\n","\n"," [[-0.00990572 -0.00831423 -0.0144679  ...  0.20512378  0.29637304\n","    0.35657892]\n","  [ 0.065937    0.07868265  0.1388286  ...  0.13779323  0.17048709\n","    0.22917762]\n","  [ 0.20942827  0.3083768   0.3611069  ...  0.15417105  0.14943762\n","    0.14847943]\n","  ...\n","  [-0.01026226  0.12369546  0.22621298 ...  0.08794262  0.06927331\n","    0.05039278]\n","  [-0.01754978  0.05998585  0.24677528 ...  0.11252077  0.14601192\n","    0.1598541 ]\n","  [ 0.10347325  0.15886857  0.22365592 ...  0.09857659  0.14692606\n","    0.34918225]]]\n","[[[4 4 4 ... 0 0 1]\n","  [4 1 1 ... 0 0 0]\n","  [1 1 1 ... 0 0 0]\n","  ...\n","  [5 1 1 ... 1 1 1]\n","  [1 1 1 ... 1 1 1]\n","  [1 1 1 ... 1 1 1]]]\n","\n","Final data shape: (4057690, 23)\n","Data Band\n","[[ 0.2223      0.2475      0.2608     ...  0.25260454 -0.12609293\n","  -0.00990572]\n"," [ 0.2207      0.2433      0.254      ...  0.25687233 -0.1292848\n","  -0.00831423]\n"," [ 0.2162      0.2382      0.255      ...  0.2557413  -0.13884722\n","  -0.0144679 ]\n"," ...\n"," [ 0.17805     0.1896      0.1966     ...  0.31347877 -0.06915577\n","   0.09857659]\n"," [ 0.1772      0.1898      0.1975     ...  0.32001793 -0.02642359\n","   0.14692606]\n"," [ 0.1681      0.1775      0.1561     ...  0.3853878   0.1446145\n","   0.34918225]]\n","Data Label\n","[4 4 4 ... 1 1 1]\n","\n","Cleaning plots folder...\n","Plots folder has been cleaned and recreated\n","\n","Membuat visualisasi perbandingan data...\n","Visualization images saved to plots/raw_data:\n","1. sentinel2_rgb_raw.png - RGB Natural Color (Before Preprocessing)\n","2. sentinel2_rgb_processed.png - RGB Natural Color (After Preprocessing)\n","3. false_color.png - False Color Composite (NIR-R-G)\n","4. spectral_indices.png - Spectral Indices (NDBI-NDVI-NDWI)\n","5. dw_classification.png - Dynamic World Classification\n","6. color_legend.png - Color Legend for Classes\n","\n","All processing complete! Results saved in 'plots' directory\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"KqH8fpodXRAU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EYp4xIXtXRME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vgxg1HM1XRO3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aYcnmdDF6enO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h15SrgRi6ep1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import zipfile\n","\n","def zip_results_folder(folder_path, output_zip_name='results.zip'):\n","    \"\"\"\n","    Zip all contents of the specified folder\n","\n","    Parameters:\n","    folder_path (str): Path to the folder containing results\n","    output_zip_name (str): Name of the output zip file\n","    \"\"\"\n","    with zipfile.ZipFile(output_zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","        for root, _, files in os.walk(folder_path):\n","            for file in files:\n","                file_path = os.path.join(root, file)\n","                arcname = os.path.relpath(file_path, folder_path)\n","                zipf.write(file_path, arcname)\n","\n","# Example usage:\n","zip_results_folder('/content/plots')"],"metadata":{"id":"7HMqA2-C6esn"},"execution_count":null,"outputs":[]}]}
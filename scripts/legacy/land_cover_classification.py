# -*- coding: utf-8 -*-
"""Land Cover Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pKX-l_iR6cKUz_5B0LKvuV3p0iEKER4o
"""

!pip install rasterio
!pip install xlsxwriter



# -*- coding: utf-8 -*-
"""Enhanced Land Cover Classification using Sentinel-2 and Dynamic World Data"""

# Import required libraries
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive
import rasterio
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    classification_report,
    confusion_matrix
)
# Di bagian import, tambahkan:
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
import lightgbm as lgb
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import seaborn as sns
import pandas as pd
from rasterio.enums import Resampling
import os
import shutil
import time
import warnings
warnings.filterwarnings('ignore')

# Constants
CLASS_NAMES = {
    0: 'Water', 1: 'Trees', 2: 'Grass',
    3: 'Flooded vegetation', 4: 'Crops',
    5: 'Shrub and scrub', 6: 'Built area',
    7: 'Bare ground', 8: 'Snow and ice'
}

COLORS = ['#419BDF', '#397D49', '#88B053', '#7A87C6',
          '#E49635', '#DFC35A', '#C4281B', '#A59B8F', '#B39FE1']

# Initial feature names will be updated after calculating indices
FEATURE_NAMES = [
    'B2 (Blue)', 'B3 (Green)', 'B4 (Red)',
    'B5 (Red Edge 1)', 'B6 (Red Edge 2)', 'B7 (Red Edge 3)',
    'B8 (NIR)', 'B8A (Red Edge 4)', 'B11 (SWIR 1)', 'B12 (SWIR 2)'
]

def calculate_enhanced_indices(data):
    """
    Calculate enhanced set of spectral indices for land cover classification.
    """
    # Extract bands
    blue = data[0]    # B2 - Blue
    green = data[1]   # B3 - Green
    red = data[2]     # B4 - Red
    nir = data[6]     # B8 - NIR
    swir1 = data[8]   # B11 - SWIR 1
    swir2 = data[9]   # B12 - SWIR 2
    redEdge1 = data[3]  # B5 - Red Edge 1
    redEdge2 = data[4]  # B6 - Red Edge 2

    epsilon = 1e-10  # Prevent division by zero

    # Basic Vegetation Indices
    ndvi = (nir - red) / (nir + red + epsilon)
    evi = 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blue + 1 + epsilon))
    savi = (1.5 * (nir - red)) / (nir + red + 0.5 + epsilon)

    # Water Indices
    ndwi = (green - nir) / (green + nir + epsilon)
    mndwi = (green - swir1) / (green + swir1 + epsilon)

    # Built-up and Bare Soil Indices
    ndbi = (swir1 - nir) / (swir1 + nir + epsilon)
    bsi = ((swir1 + red) - (nir + blue)) / ((swir1 + red) + (nir + blue) + epsilon)

    # Vegetation Red Edge Indices
    ndre = (nir - redEdge1) / (nir + redEdge1 + epsilon)
    cire = (nir / redEdge1) - 1

    # Advanced Vegetation Indices
    msavi = (2 * nir + 1 - np.sqrt((2 * nir + 1)**2 - 8 * (nir - red))) / 2
    gndvi = (nir - green) / (nir + green + epsilon)

    # Moisture and Drought Indices
    ndmi = (nir - swir1) / (nir + swir1 + epsilon)
    nbri = (nir - swir2) / (nir + swir2 + epsilon)

    # Stack all indices
    indices = np.stack([
        ndvi, evi, savi, ndwi, mndwi,
        ndbi, bsi, ndre, cire, msavi,
        gndvi, ndmi, nbri
    ])

    return indices

def update_feature_names():
    """Update global FEATURE_NAMES to include new indices."""
    global FEATURE_NAMES
    FEATURE_NAMES = [
        'B2 (Blue)', 'B3 (Green)', 'B4 (Red)',
        'B5 (Red Edge 1)', 'B6 (Red Edge 2)', 'B7 (Red Edge 3)',
        'B8 (NIR)', 'B8A (Red Edge 4)', 'B11 (SWIR 1)', 'B12 (SWIR 2)',
        'NDVI', 'EVI', 'SAVI', 'NDWI', 'MNDWI',
        'NDBI', 'BSI', 'NDRE', 'CIRE', 'MSAVI',
        'GNDVI', 'NDMI', 'NBRI'
    ]

def load_and_preprocess_data(s2_path, dw_path):
    """Load and preprocess satellite data with enhanced indices."""
    with rasterio.open(s2_path) as src:
        s2_data = src.read()
        height, width = src.height, src.width
        print(f"Sentinel-2 shape: {s2_data.shape}")

        indices = calculate_enhanced_indices(s2_data)
        print(f"Added {indices.shape[0]} spectral indices")

        s2_data = np.vstack((s2_data, indices))

    with rasterio.open(dw_path) as src:
        dw_data = src.read(
            out_shape=(1, height, width),
            resampling=Resampling.nearest
        )

    # Update feature names to include new indices
    update_feature_names()

    return s2_data, dw_data

def prepare_data(X, y):
    """Prepare data for model training."""
    X = X.reshape(X.shape[0], -1).T
    y = y.flatten()

    valid_pixels = ~np.isnan(y)
    X = X[valid_pixels]
    y = y[valid_pixels]

    return X, y

def create_visualization(plt_func):
    """Decorator for consistent plot styling."""
    def wrapper(*args, **kwargs):
        plt.figure(figsize=(12, 8))
        plt_func(*args, **kwargs)
        plt.tight_layout()
        if 'save_path' in kwargs:
            plt.savefig(kwargs['save_path'], dpi=300, bbox_inches='tight')
        plt.close()
    return wrapper

@create_visualization
def plot_ground_truth(dw_data, save_path=None):
    """Plot ground truth map."""
    custom_cmap = plt.matplotlib.colors.ListedColormap(COLORS)
    im = plt.imshow(dw_data[0], cmap=custom_cmap, vmin=0, vmax=8)

    cbar = plt.colorbar(im, ticks=range(9))
    cbar.ax.set_yticklabels([CLASS_NAMES[i] for i in range(9)])

    plt.title('Ground Truth Land Cover Classification')
    plt.axis('off')

    plt.text(0.02, 0.05, '20m/pixel', transform=plt.gca().transAxes,
             bbox=dict(facecolor='white', alpha=0.7))
    plt.annotate('N↑', xy=(0.02, 0.95), xycoords='axes fraction',
                fontsize=12, bbox=dict(facecolor='white', alpha=0.7))

@create_visualization
def plot_predictions(pipeline, X_all, dw_data, save_path=None):
    """Plot prediction map."""
    custom_cmap = plt.matplotlib.colors.ListedColormap(COLORS)
    predictions = pipeline.predict(X_all)
    prediction_map = predictions.reshape(dw_data[0].shape)

    im = plt.imshow(prediction_map, cmap=custom_cmap, vmin=0, vmax=8)
    cbar = plt.colorbar(im, ticks=range(9))
    cbar.ax.set_yticklabels([CLASS_NAMES[i] for i in range(9)])

    plt.title('Classification Results')
    plt.axis('off')

    plt.text(0.02, 0.05, '20m/pixel', transform=plt.gca().transAxes,
             bbox=dict(facecolor='white', alpha=0.7))
    plt.annotate('N↑', xy=(0.02, 0.95), xycoords='axes fraction',
                fontsize=12, bbox=dict(facecolor='white', alpha=0.7))

@create_visualization
def plot_confusion_matrix(y_test, y_pred, save_path=None):
    """Plot confusion matrix."""
    cm = confusion_matrix(y_test, y_pred)
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

    sns.heatmap(cm, annot=np.array([[f'{val:,}\n({percent:.1f}%)'
                for val, percent in zip(row, row_percent)]
                for row, row_percent in zip(cm, cm_percent)]),
                fmt='', cmap='YlOrRd',
                xticklabels=[CLASS_NAMES[i] for i in range(9)],
                yticklabels=[CLASS_NAMES[i] for i in range(9)])

    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.xticks(rotation=45, ha='right')

@create_visualization
def plot_feature_importance(model, save_path=None):
    """Plot feature importance."""
    importance_df = pd.DataFrame({
        'feature': FEATURE_NAMES,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=True)

    plt.figure(figsize=(12, 10))  # Increased figure size for more features
    plt.barh(range(len(importance_df)), importance_df['importance'], color='#2C7BB6')
    plt.yticks(range(len(importance_df)), importance_df['feature'])
    plt.xlabel('Relative Importance')
    plt.title('Feature Importance in Classification')

    for i, v in enumerate(importance_df['importance']):
        plt.text(v, i, f' {v:.3f}', va='center')

    plt.grid(axis='x', linestyle='--', alpha=0.7)

@create_visualization
def plot_accuracy_by_class(y_test, y_pred, save_path=None):
    """Plot accuracy and sample size by class."""
    metrics = {i: {
        'accuracy': (y_pred[y_test == i] == i).mean(),
        'samples': np.sum(y_test == i)
    } for i in range(9)}

    ax1 = plt.gca()
    classes = list(metrics.keys())
    accuracies = [metrics[c]['accuracy'] for c in classes]
    samples = [metrics[c]['samples'] for c in classes]

    ax1.bar(classes, accuracies, color='#2C7BB6')
    ax1.set_ylim(0, 1)
    ax1.set_ylabel('Classification Accuracy')

    ax2 = ax1.twinx()
    ax2.plot(classes, samples, 'r-o')
    ax2.set_ylabel('Number of Samples')

    plt.title('Accuracy and Sample Size by Class')
    ax1.set_xticks(classes)
    ax1.set_xticklabels([CLASS_NAMES[i] for i in classes], rotation=45, ha='right')

    for i, (acc, n) in enumerate(zip(accuracies, samples)):
        ax1.text(i, acc, f'{acc:.1%}', ha='center', va='bottom')
        ax2.text(i, n, f'{n:,}', ha='center', va='top', color='red')

    ax1.legend(['Accuracy'], loc='upper left')
    ax2.legend(['Sample size'], loc='upper right')
    plt.grid(True, alpha=0.3)

def clean_plots_folder():
    """Create plots folder if it doesn't exist."""
    if os.path.exists('plots'):
        shutil.rmtree('plots')
    os.makedirs('plots')
    print("Plots folder has been cleaned and recreated")



def train_multiple_classifiers(X, y, test_size=0.2):
    """Train and evaluate multiple classifiers."""
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=y
    )

    # Define preprocessing pipelines
    rf_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
        ('scaler', StandardScaler()),
        ('classifier', RandomForestClassifier(
            n_estimators=200,
            max_depth=25,
            min_samples_split=5,
            min_samples_leaf=2,
            class_weight='balanced',
            n_jobs=-1,
            random_state=42))
    ])

    et_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
        ('scaler', StandardScaler()),
        ('classifier', ExtraTreesClassifier(
            n_estimators=200,
            max_depth=25,
            min_samples_split=5,
            min_samples_leaf=2,
            class_weight='balanced',
            n_jobs=-1,
            random_state=42))
    ])

    lr_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(
            multi_class='multinomial',
            max_iter=500,
            class_weight='balanced',
            n_jobs=-1,
            random_state=42))
    ])

    # Fast classifier: Decision Tree
    dt_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
        ('scaler', StandardScaler()),
        ('classifier', DecisionTreeClassifier(
            max_depth=15,  # Reduced depth for speed
            min_samples_split=5,
            min_samples_leaf=2,
            class_weight='balanced',
            random_state=42))
    ])

    # Fast classifier: KNN
    knn_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
        ('scaler', StandardScaler()),
        ('classifier', KNeighborsClassifier(
            n_neighbors=5,
            weights='distance',
            algorithm='auto',
            leaf_size=30,
            n_jobs=-1))
    ])

    # Fast classifier: Naive Bayes
    nb_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
        ('scaler', StandardScaler()),
        ('classifier', GaussianNB())
    ])

    # Fast classifier: SGD
    sgd_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
        ('scaler', StandardScaler()),
        ('classifier', SGDClassifier(
            loss='modified_huber',
            penalty='l2',
            alpha=0.0001,
            max_iter=1000,
            tol=1e-3,
            n_jobs=-1,
            random_state=42))
    ])

    # Fast classifier: LightGBM
    lgb_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
        ('scaler', StandardScaler()),
        ('classifier', lgb.LGBMClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=15,
            num_leaves=31,
            n_jobs=-1,
            random_state=42))
    ])

    # Define all classifiers
    classifiers = {
        'Random Forest': rf_pipeline,
        'Extra Trees': et_pipeline,
        'Logistic Regression': lr_pipeline,
        'Decision Tree': dt_pipeline,
        # 'KNN': knn_pipeline,
        'Naive Bayes': nb_pipeline,
        'SGD': sgd_pipeline,
        'LightGBM': lgb_pipeline
    }

    results = {}
    for name, pipeline in classifiers.items():
        print(f"\nTraining {name}...")
        start_time = time.time()

        # Train classifier
        pipeline.fit(X_train, y_train)

        # Make predictions
        y_pred = pipeline.predict(X_test)

        # Calculate training time
        training_time = time.time() - start_time

        # Store results
        results[name] = {
            'classifier': pipeline,
            'y_test': y_test,
            'y_pred': y_pred,
            'training_time': training_time,
            'report': classification_report(y_test, y_pred, zero_division=0)
        }

        print(f"{name} Training Time: {training_time:.2f} seconds")
        print(f"\nClassification Report for {name}:")
        print(results[name]['report'])

    return results

@create_visualization
def plot_classifier_comparison(results, save_path=None):
    """Plot classifier performance comparison."""
    # Extract metrics
    metrics = {}
    for name, result in results.items():
        report = classification_report(
            result['y_test'],
            result['y_pred'],
            output_dict=True,
            zero_division=0
        )
        metrics[name] = {
            'Accuracy': report['accuracy'],
            'Macro F1': report['macro avg']['f1-score'],
            'Weighted F1': report['weighted avg']['f1-score'],
            'Training Time': result['training_time']
        }

    # Convert to DataFrame
    df = pd.DataFrame(metrics).T

    # Create subplots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # Performance metrics
    df[['Accuracy', 'Macro F1', 'Weighted F1']].plot(
        kind='bar', ax=ax1, width=0.8
    )
    ax1.set_title('Performance Metrics by Classifier')
    ax1.set_xlabel('Classifier')
    ax1.set_ylabel('Score')
    ax1.grid(True, alpha=0.3)
    ax1.legend(title='Metric')

    # Training time
    df['Training Time'].plot(kind='bar', ax=ax2, color='green', width=0.6)
    ax2.set_title('Training Time by Classifier')
    ax2.set_xlabel('Classifier')
    ax2.set_ylabel('Time (seconds)')
    ax2.grid(True, alpha=0.3)

    # Rotate x-labels
    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')
    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')

    # Add value labels
    for ax in [ax1, ax2]:
        for container in ax.containers:
            ax.bar_label(container, fmt='%.3f', padding=3)

    plt.tight_layout()

def shorten_name(name, max_length=31):
    """Shorten name to be Excel worksheet compatible."""
    if len(name) <= max_length:
        return name

    # Create shorter versions of common words
    replacements = {
        'Random Forest': 'RF',
        'Extra Trees': 'ET',
        'Logistic Regression': 'LR',
        'Performance': 'Perf',
        'Statistics': 'Stats',
        'Analysis': 'Anal',
        'Distribution': 'Dist',
        'Classification': 'Class',
        'Confusion Matrix': 'CM',
        'Parameters': 'Params'
    }

    result = name
    for old, new in replacements.items():
        result = result.replace(old, new)

    if len(result) > max_length:
        result = result[:max_length]

    return result


def export_analysis_to_excel(results, s2_data, dw_data, save_path='classification_analysis.xlsx'):
    """Export detailed analysis results to Excel with comprehensive analysis."""

    with pd.ExcelWriter(save_path, engine='xlsxwriter') as writer:
        workbook = writer.book
        decimal_format = workbook.add_format({'num_format': '0.00'})
        percent_format = workbook.add_format({'num_format': '0.00%'})
        thousand_format = workbook.add_format({'num_format': '#,##0;[Red]-#,##0'})

        # 1. Overall Performance Summary
        performance_metrics = {}
        for name, result in results.items():
            report = classification_report(
                result['y_test'],
                result['y_pred'],
                output_dict=True,
                zero_division=0
            )
            performance_metrics[name] = {
                'Overall Accuracy': round(float(report['accuracy']) * 100, 2),
                'Macro F1-Score': round(float(report['macro avg']['f1-score']) * 100, 2),
                'Weighted F1-Score': round(float(report['weighted avg']['f1-score']) * 100, 2),
                'Training Time (seconds)': round(result['training_time'], 2),
                'Number of Samples': len(result['y_test'])
            }

        perf_df = pd.DataFrame(performance_metrics).T
        perf_df.to_excel(writer, sheet_name='Overall_Perf')

        worksheet = writer.sheets['Overall_Perf']
        for col in ['Overall Accuracy', 'Macro F1-Score', 'Weighted F1-Score']:
            col_idx = perf_df.columns.get_loc(col) + 1
            worksheet.set_column(col_idx, col_idx, None, decimal_format)

        # 2. Per-Class Performance for each classifier
        for name, result in results.items():
            class_metrics = {}
            for i in range(9):
                mask = result['y_test'] == i

                accuracy = np.mean(result['y_test'][mask] == result['y_pred'][mask]) * 100
                prec = precision_score(result['y_test'], result['y_pred'],
                                    labels=[i], average='macro', zero_division=0) * 100
                rec = recall_score(result['y_test'], result['y_pred'],
                                 labels=[i], average='macro', zero_division=0) * 100
                f1 = f1_score(result['y_test'], result['y_pred'],
                            labels=[i], average='macro', zero_division=0) * 100

                class_metrics[CLASS_NAMES[i]] = {
                    'Total Samples': np.sum(mask),
                    'Correct Predictions': np.sum(result['y_test'][mask] == result['y_pred'][mask]),
                    'Accuracy': round(accuracy, 2),
                    'Precision': round(prec, 2),
                    'Recall': round(rec, 2),
                    'F1-Score': round(f1, 2)
                }

            sheet_name = shorten_name(f'{name}_Perf')
            class_df = pd.DataFrame(class_metrics).T
            class_df.to_excel(writer, sheet_name=sheet_name)

            worksheet = writer.sheets[sheet_name]
            for col in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:
                col_idx = class_df.columns.get_loc(col) + 1
                worksheet.set_column(col_idx, col_idx, None, decimal_format)

            for col in ['Total Samples', 'Correct Predictions']:
                col_idx = class_df.columns.get_loc(col) + 1
                worksheet.set_column(col_idx, col_idx, None, thousand_format)

        # 3. Class Distribution Analysis
        class_counts = []
        valid_counts = []

        for i in range(9):
            total_count = int(np.count_nonzero(dw_data[0] == i))
            valid_mask = (~np.isnan(dw_data[0])) & (dw_data[0] == i)
            valid_count = int(np.count_nonzero(valid_mask))

            class_counts.append(total_count)
            valid_counts.append(valid_count)

        class_distribution = pd.DataFrame({
            'Class': [CLASS_NAMES[i] for i in range(9)],
            'Total Pixels': class_counts,
            'Valid Pixels': valid_counts,
            'Invalid Pixels': [t - v for t, v in zip(class_counts, valid_counts)],
            'Validity Rate': [v/t if t > 0 else 0 for v, t in zip(valid_counts, class_counts)]
        })

        total_valid_pixels = np.sum(valid_counts)
        class_distribution['Distribution (%)'] = [(count / total_valid_pixels * 100)
                                                for count in valid_counts]

        class_distribution.to_excel(writer, sheet_name='Class_Dist')

        worksheet = writer.sheets['Class_Dist']
        for col in ['Distribution (%)', 'Validity Rate']:
            col_idx = class_distribution.columns.get_loc(col) + 1
            worksheet.set_column(col_idx, col_idx, None, decimal_format)

        for col in ['Total Pixels', 'Valid Pixels', 'Invalid Pixels']:
            col_idx = class_distribution.columns.get_loc(col) + 1
            worksheet.set_column(col_idx, col_idx, None, thousand_format)

        # 4. Confusion Matrix Analysis
        for name, result in results.items():
            # Basic Confusion Matrix
            cm = confusion_matrix(result['y_test'], result['y_pred'])
            cm_df = pd.DataFrame(cm,
                               index=[f'True_{CLASS_NAMES[i]}' for i in range(9)],
                               columns=[f'Pred_{CLASS_NAMES[i]}' for i in range(9)])

            sheet_name = shorten_name(f'{name}_CM')
            cm_df.to_excel(writer, sheet_name=sheet_name)

            # Detailed Statistics
            total_samples = len(result['y_test'])
            cm_stats = {}

            for i in range(9):
                true_pos = cm[i, i]
                false_pos = np.sum(cm[:, i]) - cm[i, i]
                false_neg = np.sum(cm[i, :]) - cm[i, i]
                true_neg = np.sum(cm) - true_pos - false_pos - false_neg

                precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0
                recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0
                specificity = true_neg / (true_neg + false_pos) if (true_neg + false_pos) > 0 else 0
                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

                cm_stats[CLASS_NAMES[i]] = {
                    'True Positives': true_pos,
                    'False Positives': false_pos,
                    'False Negatives': false_neg,
                    'True Negatives': true_neg,
                    'Precision': precision,
                    'Recall': recall,
                    'Specificity': specificity,
                    'F1-Score': f1,
                    'Support': np.sum(cm[i, :]),
                    'Predictions': np.sum(cm[:, i])
                }

            stats_sheet = shorten_name(f'{name}_Stats')
            stats_df = pd.DataFrame(cm_stats).T
            stats_df.to_excel(writer, sheet_name=stats_sheet)

            worksheet = writer.sheets[stats_sheet]
            for col in ['Precision', 'Recall', 'Specificity', 'F1-Score']:
                col_idx = stats_df.columns.get_loc(col) + 1
                worksheet.set_column(col_idx, col_idx, None, percent_format)

            for col in ['True Positives', 'False Positives', 'False Negatives',
                       'True Negatives', 'Support', 'Predictions']:
                col_idx = stats_df.columns.get_loc(col) + 1
                worksheet.set_column(col_idx, col_idx, None, thousand_format)

        # 5. Detailed Vegetation Indices Analysis
        vi_stats = {}
        vi_names = ['NDVI', 'EVI', 'SAVI', 'NDWI', 'MNDWI', 'NDBI', 'BSI',
                    'NDRE', 'CIRE', 'MSAVI', 'GNDVI', 'NDMI', 'NBRI']

        for i, vi_name in enumerate(vi_names):
            vi_data = s2_data[-(len(vi_names)-i)].flatten()
            valid_data = vi_data[~np.isnan(vi_data)]

            for class_id in range(9):
                class_mask = dw_data[0].flatten() == class_id
                class_data = vi_data[class_mask]
                valid_class_data = class_data[~np.isnan(class_data)]

                if len(valid_class_data) > 0:
                    # Calculate additional statistics
                    percentiles = np.percentile(valid_class_data, [25, 50, 75])
                    vi_stats[f'{vi_name}_{CLASS_NAMES[class_id]}'] = {
                        'Mean': np.mean(valid_class_data),
                        'Std': np.std(valid_class_data),
                        'Min': np.min(valid_class_data),
                        'Q1': percentiles[0],
                        'Median': percentiles[1],
                        'Q3': percentiles[2],
                        'Max': np.max(valid_class_data),
                        'Range': np.max(valid_class_data) - np.min(valid_class_data),
                        'IQR': percentiles[2] - percentiles[0],
                        'Samples': len(valid_class_data)
                    }

        vi_stats_df = pd.DataFrame(vi_stats).T
        vi_stats_df.to_excel(writer, sheet_name='VI_Stats')

        worksheet = writer.sheets['VI_Stats']
        for col in vi_stats_df.columns:
            if col != 'Samples':
                col_idx = vi_stats_df.columns.get_loc(col) + 1
                worksheet.set_column(col_idx, col_idx, None, decimal_format)
            else:
                col_idx = vi_stats_df.columns.get_loc(col) + 1
                worksheet.set_column(col_idx, col_idx, None, thousand_format)

        # 6. Band Statistics Analysis
        band_stats = {}
        for i, band_name in enumerate(FEATURE_NAMES[:10]):  # Original 10 bands
            band_data = s2_data[i].flatten()
            valid_data = band_data[~np.isnan(band_data)]

            for class_id in range(9):
                class_mask = dw_data[0].flatten() == class_id
                class_data = band_data[class_mask]
                valid_class_data = class_data[~np.isnan(class_data)]

                if len(valid_class_data) > 0:
                    percentiles = np.percentile(valid_class_data, [25, 50, 75])
                    band_stats[f'{band_name}_{CLASS_NAMES[class_id]}'] = {
                        'Mean': np.mean(valid_class_data),
                        'Std': np.std(valid_class_data),
                        'Min': np.min(valid_class_data),
                        'Q1': percentiles[0],
                        'Median': percentiles[1],
                        'Q3': percentiles[2],
                        'Max': np.max(valid_class_data),
                        'Range': np.max(valid_class_data) - np.min(valid_class_data),
                        'IQR': percentiles[2] - percentiles[0],
                        'Samples': len(valid_class_data)
                    }

        band_stats_df = pd.DataFrame(band_stats).T
        band_stats_df.to_excel(writer, sheet_name='Band_Stats')

        worksheet = writer.sheets['Band_Stats']
        for col in band_stats_df.columns:
            if col != 'Samples':
                col_idx = band_stats_df.columns.get_loc(col) + 1
                worksheet.set_column(col_idx, col_idx, None, decimal_format)
            else:
                col_idx = band_stats_df.columns.get_loc(col) + 1
                worksheet.set_column(col_idx, col_idx, None, thousand_format)

    print(f"\nDetailed analysis exported to {save_path}")
    return save_path

# Main execution
if __name__ == "__main__":

    # Mount Google Drive (if using Colab)
    print("Mounting Google Drive...")
    drive.mount('/content/drive', force_remount=True)

    s2_path = '/content/drive/My Drive/GEE_Exports/sentinel2_jambi_2023_allbands.tif'
    dw_path = '/content/drive/My Drive/GEE_Exports/dynamicworld_jambi_2023.tif'

    # Load and process data
    print("\nLoading and preprocessing data...")
    s2_data, dw_data = load_and_preprocess_data(s2_path, dw_path)
    X, y = prepare_data(s2_data, dw_data)
    print(f"Final data shape: {X.shape}")

    # Clean plots folder
    print("\nCleaning plots folder...")
    clean_plots_folder()

    # Train multiple classifiers
    print("\nTraining classifiers...")
    classifier_results = train_multiple_classifiers(X, y, test_size=0.99)

    # Generate comparison plot
    print("\nGenerating comparison plots...")
    plot_classifier_comparison(classifier_results, save_path='plots/classifier_comparison.png')

    # Prepare data for full prediction
    X_all = s2_data.reshape(s2_data.shape[0], -1).T

    # Generate individual plots for each classifier
    for name, result in classifier_results.items():
        print(f"\nGenerating plots for {name}...")

        # Create subfolder for each classifier
        classifier_folder = f'plots/{name.lower().replace(" ", "_")}'
        os.makedirs(classifier_folder, exist_ok=True)

        # Generate plots
        plots = [
            (plot_ground_truth, (dw_data,), 'ground_truth.png'),
            (plot_predictions, (result['classifier'], X_all, dw_data), 'predictions.png'),
            (plot_confusion_matrix, (result['y_test'], result['y_pred']), 'confusion_matrix.png'),
            (plot_accuracy_by_class, (result['y_test'], result['y_pred']), 'accuracy_by_class.png')
        ]

        # Add feature importance plot for supported classifiers
        if hasattr(result['classifier'].named_steps['classifier'], 'feature_importances_'):
            plots.append(
                (plot_feature_importance,
                 (result['classifier'].named_steps['classifier'],),
                 'feature_importance.png')
            )

        for plot_func, args, filename in plots:
            plot_func(*args, save_path=f'{classifier_folder}/{filename}')

    # Export detailed analysis to Excel
    print("\nExporting analysis results...")
    export_analysis_to_excel(
        classifier_results, s2_data, dw_data,
        save_path='plots/landcover_analysis.xlsx'
    )

    print("\nAll processing complete! Results saved in 'plots' directory")









import pandas as pd

# Data fitur dalam bentuk dictionary
feature_info = {
    'Kategori': [
        'Band Spektral', 'Band Spektral', 'Band Spektral', 'Band Spektral', 'Band Spektral',
        'Band Spektral', 'Band Spektral', 'Band Spektral', 'Band Spektral', 'Band Spektral',
        'Indeks Vegetasi Dasar', 'Indeks Vegetasi Dasar', 'Indeks Vegetasi Dasar',
        'Indeks Air', 'Indeks Air',
        'Indeks Urban & Tanah', 'Indeks Urban & Tanah',
        'Indeks Red Edge', 'Indeks Red Edge',
        'Indeks Vegetasi Lanjutan', 'Indeks Vegetasi Lanjutan',
        'Indeks Kelembaban', 'Indeks Kelembaban'
    ],
    'Nama Fitur': [
        'B2 (Blue)', 'B3 (Green)', 'B4 (Red)', 'B5 (Red Edge 1)', 'B6 (Red Edge 2)',
        'B7 (Red Edge 3)', 'B8 (NIR)', 'B8A (Red Edge 4)', 'B11 (SWIR 1)', 'B12 (SWIR 2)',
        'NDVI', 'EVI', 'SAVI',
        'NDWI', 'MNDWI',
        'NDBI', 'BSI',
        'NDRE', 'CIRE',
        'MSAVI', 'GNDVI',
        'NDMI', 'NBRI'
    ],
    'Rumus': [
        '-', '-', '-', '-', '-',
        '-', '-', '-', '-', '-',
        '(NIR - Red) / (NIR + Red)',
        '2.5 × ((NIR - Red) / (NIR + 6×Red - 7.5×Blue + 1))',
        '1.5 × (NIR - Red) / (NIR + Red + 0.5)',
        '(Green - NIR) / (Green + NIR)',
        '(Green - SWIR1) / (Green + SWIR1)',
        '(SWIR1 - NIR) / (SWIR1 + NIR)',
        '((SWIR1 + Red) - (NIR + Blue)) / ((SWIR1 + Red) + (NIR + Blue))',
        '(NIR - RedEdge1) / (NIR + RedEdge1)',
        '(NIR / RedEdge1) - 1',
        '(2×NIR + 1 - √((2×NIR + 1)² - 8×(NIR - Red))) / 2',
        '(NIR - Green) / (NIR + Green)',
        '(NIR - SWIR1) / (NIR + SWIR1)',
        '(NIR - SWIR2) / (NIR + SWIR2)'
    ],
    'Deskripsi & Kegunaan': [
        'Penetrasi air & aerosol',
        'Reflektansi vegetasi hijau',
        'Absorpsi klorofil',
        'Transisi red-NIR untuk vegetasi',
        'Indikator status vegetasi',
        'Transisi red-NIR untuk vegetasi',
        'Reflektansi vegetasi kuat',
        'Indikator biomassa',
        'Kelembaban tanah/vegetasi',
        'Kelembaban tanah/vegetasi',
        'Indeks vegetasi standar, kesehatan vegetasi',
        'Indeks vegetasi yang ditingkatkan, mengurangi pengaruh atmosfer',
        'Indeks vegetasi yang disesuaikan dengan tanah',
        'Indeks air standar, deteksi badan air',
        'Indeks air yang dimodifikasi, lebih sensitif terhadap air',
        'Indeks area terbangun, deteksi urban',
        'Indeks tanah terbuka, deteksi lahan kosong',
        'Indeks vegetasi menggunakan red edge',
        'Indeks klorofil menggunakan red edge',
        'Indeks vegetasi yang dimodifikasi untuk tanah',
        'Indeks vegetasi menggunakan green',
        'Indeks kelembaban, monitoring stress vegetasi',
        'Indeks area terbakar, deteksi kerusakan'
    ]
}

# Buat DataFrame
df = pd.DataFrame(feature_info)

# Export ke Excel dengan format
with pd.ExcelWriter('feature_information.xlsx', engine='xlsxwriter') as writer:
    # Convert DataFrame ke Excel
    df.to_excel(writer, sheet_name='Feature List', index=False)

    # Dapatkan workbook dan worksheet
    workbook = writer.book
    worksheet = writer.sheets['Feature List']

    # Format untuk header
    header_format = workbook.add_format({
        'bold': True,
        'fg_color': '#D7E4BC',
        'border': 1,
        'align': 'center',
        'valign': 'vcenter',
        'text_wrap': True
    })

    # Format untuk kategori
    category_format = workbook.add_format({
        'fg_color': '#F2F2F2',
        'border': 1,
        'align': 'left',
        'valign': 'vcenter',
        'text_wrap': True
    })

    # Format untuk konten
    content_format = workbook.add_format({
        'border': 1,
        'align': 'left',
        'valign': 'vcenter',
        'text_wrap': True
    })

    # Set column widths
    worksheet.set_column('A:A', 20)  # Kategori
    worksheet.set_column('B:B', 15)  # Nama Fitur
    worksheet.set_column('C:C', 40)  # Rumus
    worksheet.set_column('D:D', 40)  # Deskripsi

    # Format header
    for col_num, value in enumerate(df.columns.values):
        worksheet.write(0, col_num, value, header_format)

    # Format rows
    for row_num in range(1, len(df) + 1):
        # Apply category format
        worksheet.write(row_num, 0, df['Kategori'].iloc[row_num-1], category_format)
        # Apply content format to other columns
        for col_num in range(1, len(df.columns)):
            worksheet.write(row_num, col_num, df.iloc[row_num-1, col_num], content_format)

    # Set row height
    worksheet.set_default_row(30)
    worksheet.set_row(0, 40)  # Header row height

print("Feature information has been exported to 'feature_information.xlsx'")





import os
import zipfile

def zip_results_folder(folder_path, output_zip_name='results.zip'):
    """
    Zip all contents of the specified folder

    Parameters:
    folder_path (str): Path to the folder containing results
    output_zip_name (str): Name of the output zip file
    """
    with zipfile.ZipFile(output_zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, _, files in os.walk(folder_path):
            for file in files:
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, folder_path)
                zipf.write(file_path, arcname)

# Example usage:
zip_results_folder('/content/plots')